{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Use Case and Model  Life cycle Governance with SageMaker Model Registry resource sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## ML Flow Experimentation with Shared Model Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "This notebook has been tested in Amazon SageMaker Studio with the SageMaker Distribution container 1.9 and Python 3 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb989c-49aa-468b-a687-ec4bb55b0d34",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 1. Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d67713-589e-4c3e-aebd-8ee1817f36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import os\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "prefix = \"mlflow-credit-risk\"\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "s3_root_folder = f\"s3://{bucket_name}/{prefix}\"\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role(sess)\n",
    "print (f\"Your Amazon SageMaker Execution role is: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "**Access Model Package Groups in Shared Services account**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ef486-8ac9-4895-b7d2-6d7a5eafa557",
   "metadata": {},
   "source": [
    "To be able to access Model Package Groups in a Shared Services AWS account, you'll need the following permissions assigned to the SageMaker execution role. \n",
    "Replace **\\<YOUR_AWS_ACCOUNT\\>** with your own AWS Account number. Replace **\\<SHARED_SERVICES_ACCOUNT\\>** with the Account number of the Shared Services account.\n",
    "```json\n",
    "\n",
    "\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ram:GetResourceShareInvitations\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:ram:us-east-1:<YOUR_AWS_ACCOUNT>:resource-share-invitation/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ram:AcceptResourceShareInvitation\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:ram:us-east-1:<SHARED_SERVICES_ACCOUNT>:resource-share-invitation/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Before you get started, check if there are any pending invitations from the shared services account \n",
    "and accept them. \n",
    "This will allow you to discover share model package groups and register your model versions against them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_client = boto3.client('ram')\n",
    "response = ram_client.get_resource_share_invitations()\n",
    "pending_invitations = []\n",
    "# Review all pending invitations\n",
    "for i in response['resourceShareInvitations']:\n",
    "    if i['status'] == \"PENDING\":\n",
    "        pending_invitations.append(i)\n",
    "print(pending_invitations,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept the resource share invitation from shared services account\n",
    "if pending_invitations:\n",
    "    response = ram_client.accept_resource_share_invitation(resourceShareInvitationArn=pending_invitations[0]['resourceShareInvitationArn'])\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "To set up and manage an MLflow tracking server, as well as work with managed MLflow experiments, you'll need the following permissions assigned to the SageMaker execution role\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"Version\": \"2012-10-17\",\n",
    "\t\"Statement\": [\n",
    "\t\t{\n",
    "\t\t\t\"Sid\": \"VisualEditor0\",\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Action\": [\n",
    "\t\t\t\t\"sagemaker:DeleteMlflowTrackingServer\",\n",
    "\t\t\t\t\"sagemaker:StartMlflowTrackingServer\",\n",
    "\t\t\t\t\"sagemaker:CreatePresignedMlflowTrackingServerUrl\",\n",
    "\t\t\t\t\"sagemaker:UpdateMlflowTrackingServer\",\n",
    "\t\t\t\t\"sagemaker:CreateMlflowTrackingServer\",\n",
    "\t\t\t\t\"sagemaker:StopMlflowTrackingServer\"\n",
    "\t\t\t],\n",
    "\t\t\t\"Resource\": \"*\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"Sid\": \"VisualEditor1\",\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Action\": [\n",
    "\t\t\t\t\"sagemaker-mlflow:*\"\n",
    "\t\t\t],\n",
    "\t\t\t\"Resource\": \"*\"\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f083e1-2b2e-4d5c-a26b-7ac9f7e44e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_METADATA_FILE = \"/opt/ml/metadata/resource-metadata.json\"\n",
    "domain_id = 'default'\n",
    "if os.path.exists(NOTEBOOK_METADATA_FILE):\n",
    "    with open(NOTEBOOK_METADATA_FILE, \"rb\") as f:\n",
    "        metadata = json.loads(f.read())\n",
    "        domain_id = metadata.get('DomainId')\n",
    "        space_name = metadata.get('SpaceName')\n",
    "        print(f\"SageMaker domain id: {domain_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_running_mlflow_server(sagemaker_client, status_filter=['Created', 'Creating']):\n",
    "    for status in status_filter:\n",
    "        servers = sagemaker_client.list_mlflow_tracking_servers(TrackingServerStatus=status)['TrackingServerSummaries']\n",
    "        if servers:\n",
    "            server = servers[0]\n",
    "            print(f\"Found an MLflow server {server['TrackingServerArn']} in the status '{status}'.\")\n",
    "            return server['TrackingServerArn'], server['TrackingServerName']\n",
    "    print(\"No MLflow servers found.\")\n",
    "    return None, None\n",
    "\n",
    "def create_mlflow_server(sagemaker_client, bucket_name, sm_role, domain_id):\n",
    "    \"\"\"\n",
    "    Creates a new MLflow server and returns its ARN and name.\n",
    "    \"\"\"\n",
    "    timestamp = strftime('%d-%H-%M-%S', gmtime())\n",
    "    mlflow_name = f\"mlflow-{domain_id}-{timestamp}\"\n",
    "    response = sagemaker_client.create_mlflow_tracking_server(\n",
    "        TrackingServerName=mlflow_name,\n",
    "        ArtifactStoreUri=f\"s3://{bucket_name}/mlflow/{timestamp}\",\n",
    "        RoleArn=sm_role,\n",
    "        AutomaticModelRegistration=True,\n",
    "    )\n",
    "\n",
    "    mlflow_arn = response['TrackingServerArn']\n",
    "    print(f\"Server creation request succeeded. The server {mlflow_arn} is being created.\")\n",
    "    return mlflow_arn, mlflow_name\n",
    "\n",
    "# Get a running MLflow server or create a new one if none exists\n",
    "mlflow_arn, mlflow_name = get_running_mlflow_server(sagemaker_client)\n",
    "if not mlflow_arn:\n",
    "    mlflow_arn, mlflow_name = create_mlflow_server(sagemaker_client, bucket_name, sm_role, domain_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The code was adapted from this repository https://github.com/aws-samples/amazon-sagemaker-credit-risk-prediction-explainability-bias-detection/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "S3Downloader.download(\n",
    "    \"s3://sagemaker-sample-files/datasets/tabular/uci_statlog_german_credit_data/SouthGermanCredit.asc\",\n",
    "    \"data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_columns = [\n",
    "    \"status\",\n",
    "    \"duration\",\n",
    "    \"credit_history\",\n",
    "    \"purpose\",\n",
    "    \"amount\",\n",
    "    \"savings\",\n",
    "    \"employment_duration\",\n",
    "    \"installment_rate\",\n",
    "    \"personal_status_sex\",\n",
    "    \"other_debtors\",\n",
    "    \"present_residence\",\n",
    "    \"property\",\n",
    "    \"age\",\n",
    "    \"other_installment_plans\",\n",
    "    \"housing\",\n",
    "    \"number_credits\",\n",
    "    \"job\",\n",
    "    \"people_liable\",\n",
    "    \"telephone\",\n",
    "    \"foreign_worker\",\n",
    "    \"credit_risk\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\n",
    "    \"data/SouthGermanCredit.asc\",\n",
    "    names=credit_columns,\n",
    "    header=0,\n",
    "    sep=r\" \",\n",
    "    engine=\"python\",\n",
    "    na_values=\"?\",\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = training_data.sample(frac=0.1, random_state=42)\n",
    "test_data = test_data.drop(\"credit_risk\", axis=1)\n",
    "test_columns = [\n",
    "    \"status\",\n",
    "    \"duration\",\n",
    "    \"credit_history\",\n",
    "    \"purpose\",\n",
    "    \"amount\",\n",
    "    \"savings\",\n",
    "    \"employment_duration\",\n",
    "    \"installment_rate\",\n",
    "    \"personal_status_sex\",\n",
    "    \"other_debtors\",\n",
    "    \"present_residence\",\n",
    "    \"property\",\n",
    "    \"age\",\n",
    "    \"other_installment_plans\",\n",
    "    \"housing\",\n",
    "    \"number_credits\",\n",
    "    \"job\",\n",
    "    \"people_liable\",\n",
    "    \"telephone\",\n",
    "    \"foreign_worker\",\n",
    "]\n",
    "\n",
    "training_data.to_csv(\"train.csv\", index=False, header=True, columns=credit_columns)\n",
    "test_data.to_csv(\"test.csv\", index=False, header=True, columns=test_columns)\n",
    "\n",
    "# save the datasets in S3 for future use\n",
    "train_s3_url = sagemaker.Session().upload_data(\n",
    "    path=\"train.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=f\"{prefix}/input\"\n",
    ")\n",
    "print(f\"Upload the dataset to {train_s3_url}\")\n",
    "\n",
    "test_s3_url = sagemaker.Session().upload_data(\n",
    "    path=\"test.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=f\"{prefix}/input\"\n",
    ")\n",
    "print(f\"Upload the dataset to {test_s3_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 3. Process the data with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "experiment_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "registered_model_name = f\"credit-risk-model-{experiment_suffix}\"\n",
    "experiment_name = f\"credit-risk-model-experiment-{experiment_suffix}\"\n",
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import sklearn\n",
    "import joblib\n",
    "import mlflow\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sagemaker.remote_function import remote\n",
    "\n",
    "\n",
    "@remote(s3_root_uri=f\"s3://{bucket_name}/{prefix}\", dependencies=f\"requirements.txt\", instance_type=\"ml.m5.large\")\n",
    "def preprocess(df, experiment_name, mlflow_arn, bucket_name, prefix, run_id=None):\n",
    "    \"\"\"\n",
    "    Preprocess the input data and split it into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input data.\n",
    "        experiment_name (str): Name of the MLflow experiment.\n",
    "        run_id (str, optional): MLflow run ID. If not provided, a new run will be created.\n",
    "        mlflow_arn (str, optional): MLflow tracking URI.\n",
    "        s3_root_folder (str, optional): S3 root folder for remote execution.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training and validation features and labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(mlflow_arn)\n",
    "        suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "        mlflow.set_experiment(experiment_name=experiment_name if experiment_name else f\"credit-risk-model-experiment-{suffix}\")\n",
    "        run = mlflow.start_run(run_id=run_id) if run_id else mlflow.start_run(run_name=f\"remote-processing-{suffix}\", nested=True)\n",
    "\n",
    "        output_path = \"/opt/ml/output/data\"\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        print(\"Reading input data\")\n",
    "        model_dataset = mlflow.data.from_pandas(df)\n",
    "        mlflow.log_input(model_dataset, context=\"model_dataset\")\n",
    "\n",
    "        print(\"Performing one-hot encoding\")\n",
    "        categorical_cols = [\n",
    "            \"credit_history\",\n",
    "            \"purpose\",\n",
    "            \"personal_status_sex\",\n",
    "            \"other_debtors\",\n",
    "            \"property\",\n",
    "            \"other_installment_plans\",\n",
    "            \"housing\",\n",
    "            \"job\",\n",
    "            \"telephone\",\n",
    "            \"foreign_worker\",\n",
    "        ]\n",
    "        transformer = make_column_transformer(\n",
    "            (OneHotEncoder(sparse_output=False), categorical_cols),\n",
    "            remainder=\"passthrough\",\n",
    "        )\n",
    "\n",
    "        print(\"Preparing features and labels\")\n",
    "        X = df.drop(\"credit_risk\", axis=1)\n",
    "        y = df[\"credit_risk\"]\n",
    "\n",
    "        print(\"Building scikit-learn transformer\")\n",
    "        featurizer_model = transformer.fit(X)\n",
    "        features = featurizer_model.transform(X)\n",
    "        labels = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        split_ratio = 0.3\n",
    "        print(f\"Splitting data into train and validation sets with ratio {split_ratio}\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            features, labels, test_size=split_ratio, random_state=0\n",
    "        )\n",
    "\n",
    "        print(f\"Train features shape after preprocessing: {X_train.shape}\")\n",
    "        print(f\"Validation features shape after preprocessing: {X_val.shape}\")\n",
    "\n",
    "        mlflow.log_params({\"train_shape\": X_train.shape, \"val_shape\": X_val.shape})\n",
    "\n",
    "        train_features_path = os.path.join(output_path, \"train_features.csv\")\n",
    "        print(f\"Saving training features to {train_features_path}\")\n",
    "        pd.DataFrame(X_train).to_csv(train_features_path, header=False, index=False)\n",
    "\n",
    "        train_labels_path = os.path.join(output_path, \"train_labels.csv\")\n",
    "        print(f\"Saving training labels to {train_labels_path}\")\n",
    "        pd.DataFrame(y_train).to_csv(train_labels_path, header=False, index=False)\n",
    "\n",
    "        val_features_path = os.path.join(output_path, \"val_features.csv\")\n",
    "        print(f\"Saving validation features to {val_features_path}\")\n",
    "        pd.DataFrame(X_val).to_csv(val_features_path, header=False, index=False)\n",
    "\n",
    "        val_labels_path = os.path.join(output_path, \"val_labels.csv\")\n",
    "        print(f\"Saving validation labels to {val_labels_path}\")\n",
    "        pd.DataFrame(y_val).to_csv(val_labels_path, header=False, index=False)\n",
    "\n",
    "        model_dir = \"/opt/ml/model\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        model_path = os.path.join(model_dir, \"model.joblib\")\n",
    "        model_output_path = os.path.join(model_dir, \"model.tar.gz\")\n",
    "\n",
    "        print(f\"Saving featurizer model to {model_output_path}\")\n",
    "        joblib.dump(featurizer_model, model_path)\n",
    "        with tarfile.open(model_output_path, \"w:gz\") as tar:\n",
    "            tar.add(model_path, arcname=\"model.joblib\")\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=featurizer_model,\n",
    "            artifact_path=\"processing/model\",\n",
    "            registered_model_name=\"sk-learn-model\",\n",
    "        )  \n",
    "        return X_train, X_val, y_train, y_val\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Exception in processing script: {e}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", names=None, header=0, sep=\",\")\n",
    "X_train, X_val, y_train, y_val = preprocess(df, experiment_name, mlflow_arn, bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 4. Model training with SageMaker training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle as pkl\n",
    "import os\n",
    "import mlflow\n",
    "import tarfile\n",
    "\n",
    "@remote(s3_root_uri=f\"s3://{bucket_name}/{prefix}\", dependencies=f\"requirements.txt\", instance_type=\"ml.m5.large\")\n",
    "def train(X, val_X, y, val_y, num_round, params, mlflow_arn, experiment_name,run_id=None):\n",
    "    output_path = \"/opt/ml/model\"\n",
    "    mlflow.set_tracking_uri(mlflow_arn)\n",
    "    mlflow.autolog()\n",
    "    \n",
    "    suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "    mlflow.set_experiment(experiment_name=experiment_name if experiment_name else f\"credit-risk-model-experiment-{suffix}\")\n",
    "    run = mlflow.start_run(run_id=run_id) if run_id else mlflow.start_run(run_name=f\"remote-training-{suffix}\", nested=True)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        print(f\"Directory '{output_path}' created successfully.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory '{output_path}': {e}\")\n",
    "        \n",
    "    dtrain = xgboost.DMatrix(X, label=y)\n",
    "    dval = xgboost.DMatrix(val_X, label=val_y)\n",
    "\n",
    "    dtrain = xgboost.DMatrix(X, label=y)\n",
    "    dval = xgboost.DMatrix(val_X, label=val_y)\n",
    "\n",
    "    watchlist = [(dtrain, \"train\"), (dval, \"validation\")]\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    print(\"Training the model\")\n",
    "    evaluation__results = {}\n",
    "    bst = xgboost.train(\n",
    "        params=params, dtrain=dtrain, evals=watchlist, num_boost_round=num_round\n",
    "    )\n",
    "    pkl.dump(bst, open(output_path + \"/model.bin\", \"wb\"))\n",
    "\n",
    "     # Compress the model.bin artifact to a tar file\n",
    "    tar_filename = f\"{output_path}/model.tar.gz\"\n",
    "    with tarfile.open(tar_filename, \"w:gz\") as tar:\n",
    "        tar.add(f\"{output_path}/model.bin\", arcname=\"model.bin\")\n",
    "\n",
    "    # Upload the compressed model to S3\n",
    "    # s3_client = boto3.client(\"s3\")\n",
    "    # s3_key = f\"{s3_prefix}/model_{run.info.run_id}.tar.gz\"\n",
    "    # s3_client.upload_file(tar_filename, s3_bucket, s3_key)\n",
    "\n",
    "    mlflow.log_artifact(local_path=tar_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.1\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"silent\": \"1\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"eval_metric\": \"auc\"\n",
    "}\n",
    "num_round = 50\n",
    "\n",
    "train(X_train, X_val, y_train, y_val,num_round, hyperparameters, mlflow_arn, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 5. Register your the candidate model to the model registry in the shared services account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Now register the trained model in the MLflow model registry. The model is also automatically registered in the SageMaker model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.entities import ViewType\n",
    "\n",
    "run_filter = f\"\"\"\n",
    "attributes.run_name LIKE \"%training%\"\n",
    "attributes.status = 'FINISHED'\n",
    "\"\"\"\n",
    "\n",
    "runs_with_filter = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name],\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    filter_string=run_filter,\n",
    "    order_by=[\"metrics.`validation-auc` DESC\"],\n",
    ")\n",
    "best_run = runs_with_filter[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_uri = best_run['artifact_uri'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_client.list_model_package_groups(CrossAccountFilterOption=\"CrossAccount\")\n",
    "model_package_group_arn = response['ModelPackageGroupSummaryList'][0]['ModelPackageGroupArn']\n",
    "print(model_package_group_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{artifact_uri}/model/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "modelpackage_inference_specification =  {\n",
    "    \"InferenceSpecification\": {\n",
    "      \"Containers\": [\n",
    "         {\n",
    "            \"Image\": \"885854791233.dkr.ecr.us-east-1.amazonaws.com/sagemaker-distribution-prod@sha256:9e7622bbe2f3ee9dd516797bfe3ed310983b96190eeefbdeeeea69519d3946fe\",\n",
    "    \t    \"ModelDataUrl\": f\"{artifact_uri}/model.tar.gz\"\n",
    "         }\n",
    "      ],\n",
    "      \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "      \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "   },\n",
    "    \"ModelPackageGroupName\" : model_package_group_arn,\n",
    "    \"ModelPackageDescription\" : \"Model to detect credit risk\",\n",
    "    \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "}\n",
    "\n",
    "model_package_group_name = \"model-group-\" + str(round(time.time()))\n",
    "\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\" : model_package_group_name,\n",
    "    \"ModelPackageDescription\" : \"Model to detect credit risk\",\n",
    "    \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "}\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "\n",
    "create_model_package_response = sagemaker_client.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "print('ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_package_response = sagemaker_client.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "print('ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
