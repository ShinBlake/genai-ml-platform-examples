{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Langfuse Tracing\n",
    "\n",
    "In this lab, we will learn how to use Langfuse Tracing to log and analyze the execution of your LLM applications. The Langfuse is self-hosted on AWS but there is a cloud version available.\n",
    "\n",
    "[Tracing](https://langfuse.com/docs/tracing) in Langfuse is a way to log and analyze the execution of your LLM applications. The following reference provides a detailed overview of the data model used. It is inspired by OpenTelemetry.\n",
    "\n",
    "\n",
    "## Traces and Observations:\n",
    "A trace typically represents a single request or operation. It contains the overall input and output of the function, as well as metadata about the request, such as the user, the session, and tags. Usually, a trace corresponds to a single api call of an application.\n",
    "\n",
    "Each trace can contain multiple observations to log the individual steps of the execution.\n",
    "\n",
    "- Observations are of different types:\n",
    "    - Events are the basic building blocks. They are used to track discrete events in a trace.\n",
    "    - Spans represent durations of units of work in a trace.\n",
    "    - Generations are spans used to log generations of AI models. They contain additional attributes about the model, the prompt, and the completion. For generations, token usage and costs are automatically calculated.\n",
    "- Observations can be nested.\n",
    "\n",
    "![Trace and Observations](./images/trace-observation.png)\n",
    "[Source](https://langfuse.com/docs/tracing)\n",
    "\n",
    "<!-- Adding an screenshot of the trace and observations -->\n",
    "\n",
    "## Sessions\n",
    "Optionally, traces can be grouped into sessions. Sessions are used to group traces that are part of the same user interaction. A common example is a thread in a chat interface.\n",
    "Please refer to the [Sessions documentation](https://langfuse.com/docs/sessions) to add sessions to your traces.\n",
    "![Trace and Sessions](./images/trace-sessions.png)\n",
    "[Source](https://langfuse.com/docs/tracing)\n",
    "<!-- Adding an screenshot of the trace and sessions -->\n",
    "\n",
    "\n",
    "## Scores[PENDING]: This better to be introduced in the context of evaluationd section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Setting\n",
    "\n",
    "Please make sure you have completed the prerequisites to setup the Langfuse project and API keys otherwise xxxx;\n",
    "Also check custom model pricing otherwise please add the custom model pricing to the Langfuse project. Link to the section;\n",
    "\n",
    "### Set Langfuse Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" # Your Langfuse project secret key\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" # Your Langfuse project public key\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://xx.cloud.langfuse.com\" # Region-specific Langfuse domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Python package & import all the necessary packages\n",
    "\n",
    "We will use the langfuse, boto3:\n",
    "- The langfuse Python SDK along with the self-hosting deployment to debug and improve LLM applications by tracing model invocations, managing prompts / models configurations and running evaluations.\n",
    "- The boto3 SDK to interact with models on Amazon Bedrock or Amazon SageMaker.\n",
    "- Langfuse python SDK \n",
    "\n",
    "\n",
    "Run the following command to install the required Python SDKs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langfuse==2.54.1 boto3==1.35.70 litellm==1.52.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary packages\n",
    "import boto3\n",
    "from typing import List, Dict, Optional, Any\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "from botocore.exceptions import ClientError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# used to access Bedrock configuration\n",
    "# region has to be in us-west-2 for this lab\n",
    "bedrock = boto3.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    " \n",
    "# used to invoke the Bedrock Converse API\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# Check which models are available in your account\n",
    "models = bedrock.list_inference_profiles()\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "  print(model[\"inferenceProfileName\"] + \" - \" + model[\"inferenceProfileId\"])\n",
    "#  Coverage, log level, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bedrock Converse API\n",
    "You can use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an Amazon Bedrock model. For example, you can create a chat bot that maintains a conversation over many turns and uses a persona or tone customization that is unique to your needs, such as a helpful technical support assistant.\n",
    "\n",
    "To use the Converse API, you use the Converse or ConverseStream (for streaming responses) operations to send messages to a model. It is possible to use the existing base inference operations (InvokeModel or InvokeModelWithResponseStream) for conversation applications. However, we recommend using the Converse API as it provides consistent API, that works with all Amazon Bedrock models that support messages. This means you can write code once and use it with different models. Should a model have unique inference parameters, the Converse API also allows you to pass those unique parameters in a model specific structure.\n",
    "\n",
    "For more details, please refer to the [Carry out a conversation with the Converse API operations](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n",
    "\n",
    "#### Build a wrapper for Bedrock SDK with Converse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(as_type=\"generation\", name=\"Bedrock Converse\")\n",
    "def bedrock_converse_langfuse_wrapper(**kwargs):\n",
    "    # 1. Extract the conversation history from the kwargs\n",
    "    kwargs_clone = kwargs.copy()\n",
    "    message = kwargs_clone.pop(\"message\", None)\n",
    "    modelId = kwargs_clone.pop(\"modelId\", None)\n",
    "    model_parameters = {\n",
    "        **kwargs_clone.pop(\"inferenceConfig\", {}),\n",
    "        **kwargs_clone.pop(\"additionalModelRequestFields\", {}),\n",
    "    }\n",
    "    langfuse_context.update_current_observation(\n",
    "        input=message,\n",
    "        model=modelId,\n",
    "        model_parameters=model_parameters,\n",
    "        metadata= kwargs_clone\n",
    "    )\n",
    "    \n",
    "    # 2. Invoke the Bedrock Converse API\n",
    "    try:\n",
    "        response = bedrock_runtime.converse(**kwargs)\n",
    "    except (ClientError, Exception) as e:\n",
    "        error_message = f\"Error invoking Bedrock Converse API: Can't invoke 'modelId' with reason {e}\"\n",
    "        langfuse_context.update_current_observation( level=\"ERROR\", status_message=error_message)\n",
    "        return {\n",
    "            \"error\": error_message,\n",
    "            \"type\": type(e).__name__,\n",
    "            \"statusCode\": 500\n",
    "        }\n",
    "    # 3. handle the response\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    langfuse_context.update_current_observation(\n",
    "        output=response_text,\n",
    "        usage = {\n",
    "            \"input\": response[\"usage\"][\"inputTokens\"],\n",
    "            \"output\": response[\"usage\"][\"outputTokens\"],\n",
    "            \"total\": response[\"usage\"][\"totalTokens\"]\n",
    "        },\n",
    "        metadata= {\n",
    "            \"ResponseMetadata\": response[\"ResponseMetadata\"],\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "        \"response\": response_text,\n",
    "        \"statusCode\": 200\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bedrock_converse_langfuse_wrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#system prompt??\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#user prompt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_message}]}\n\u001b[1;32m     23\u001b[0m ]\n\u001b[0;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock_converse_langfuse_wrapper\u001b[49m(\n\u001b[1;32m     26\u001b[0m     modelId\u001b[38;5;241m=\u001b[39mmodelId,\n\u001b[1;32m     27\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessages\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bedrock_converse_langfuse_wrapper' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "modelId = \"us.amazon.nova-pro-v1:0\"\n",
    "\n",
    "# Converesation according to AWS spec including prompting + history\n",
    "user_message = \"\"\"You will be acting as an AI personal finance advisor named Alex, created by the company SmartFinance Advisors. Your goal is to provide financial advice and guidance to users. You will be replying to users who are on the SmartFinance Advisors site and who will be confused if you don't respond in the character of Alex.\n",
    " \n",
    "Here is the conversational history (between the user and you) prior to the question. It could be empty if there is no history:\n",
    "<history>\n",
    "User: Hi Alex, I'm really looking forward to your advice!\n",
    "Alex: Hello! I'm Alex, your AI personal finance advisor from SmartFinance Advisors. How can I assist you with your financial goals today?\n",
    "</history>\n",
    " \n",
    "Here are some important rules for the interaction:\n",
    "-  Always stay in character, as Alex, an AI from SmartFinance Advisors.\n",
    "-  If you are unsure how to respond, say \"I'm sorry, I didn't quite catch that. Could you please rephrase your question?\"\n",
    "\"\"\"\n",
    "\n",
    "#system prompt??\n",
    "\n",
    "#user prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \n",
    "    \"content\": [{\"text\": user_message}]}\n",
    "]\n",
    "\n",
    "response = bedrock_converse_langfuse_wrapper(\n",
    "    modelId=modelId,\n",
    "    message=messages\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 1. Add a system prompt\n",
    "# adding tags\n",
    "# adding guardrails\n",
    "# explain metadata\n",
    "# adding users \n",
    "# adding sessions???\n",
    "# adding prompt management\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
