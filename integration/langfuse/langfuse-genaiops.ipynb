{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "## (Optional) Self-hosting\n",
    "Follow [this blog](https://community.aws/content/2klaPSIl7P4aAd2IJRWeS0NyVtn/deploying-langfuse-on-amazon-ecs-with-aws-fargate-a-step-by-step-guide-using-aws-cdk) to deploy Langfuse v2 on Amazon ECS with AWS Fargate.\n",
    "\n",
    "## Project Setup\n",
    "1. Go to your Langfuse domain\n",
    "2. Create a new project\n",
    "3. Create new API credentials in the project settings and save the API keys.\n",
    "\n",
    "Define them directly in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" # Your Langfuse project secret key\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" # Your Langfuse project public key\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://xx.cloud.langfuse.com\" # Region-specific Langfuse domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Or use the `.env` file:\n",
    "\n",
    "   ```bash\n",
    "   LANGFUSE_SECRET_KEY=sk-lf-... # Your Langfuse project secret key\n",
    "   LANGFUSE_PUBLIC_KEY=pk-lf-... # Your Langfuse project public key\n",
    "   LANGFUSE_HOST=https://xxx.xxx.awsapprunner.com # App Runner domain\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Langfuse documentation for more details: https://langfuse.com/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Dependencies\n",
    "\n",
    "We will use the `langfuse`, `boto3` and `litellm` Python packages. Specifically, we will use:\n",
    "\n",
    "- The `langfuse` SDK along with the public or self-hosting deployment to debug and improve LLM applications by tracing model invocations, managing prompts / models configurations and running evaluations.\n",
    "- The `boto3` SDK to interact with models on Amazon Bedrock or Amazon SageMaker.\n",
    "- (Optional) The `litellm` SDK to route requests to different LLM models with advanced load balancing and fallback, as well as standardizing the responses for chat, streaming, function calling and more.\n",
    "\n",
    "Note that you can also use other frameworks like LangChain or implement your own proxy instead of using `litellm`.\n",
    "\n",
    "Run the following command to install the required Python SDKs:\n",
    "\n",
    "```bash\n",
    "pip install langfuse==2.54.1 boto3==1.35.70 litellm==1.52.16\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Gateway Options\n",
    "Choose one of the following options to invoke the LLM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Bedrock Converse API\n",
    "When only using Amazon Bedrock models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# used to access Bedrock configuration\n",
    "bedrock = boto3.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    " \n",
    "# used to invoke the Bedrock Converse API\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# Check which models are available in your account\n",
    "models = bedrock.list_inference_profiles()\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "  print(model[\"inferenceProfileName\"] + \" - \" + model[\"inferenceProfileId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from langfuse.client import PromptClient\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "\n",
    "@observe(as_type=\"generation\", name=\"Bedrock Converse\")\n",
    "def fn(\n",
    "    messages: List[Dict[str, Any]],\n",
    "    prompt: Optional[PromptClient] = None,\n",
    "    **kwargs,\n",
    ") -> str | None:\n",
    "    # 1. extract model metadata\n",
    "    modelId = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    inferenceConfig = {\"maxTokens\": 500, \"temperature\": 0.1}\n",
    "    additionalModelRequestFields = {\"top_k\": 250}\n",
    "\n",
    "    model_parameters = {**inferenceConfig, **additionalModelRequestFields}\n",
    "\n",
    "    langfuse_context.update_current_observation(\n",
    "        input=messages,\n",
    "        model=modelId,\n",
    "        model_parameters=model_parameters,\n",
    "        prompt=prompt,\n",
    "        metadata=kwargs,\n",
    "    )\n",
    "\n",
    "    # Extract the system prompts from the messages and convert them to the format expected by the Bedrock Converse API\n",
    "    system_prompts = [\n",
    "        {\"text\": message[\"content\"]}\n",
    "        for message in messages\n",
    "        if message[\"role\"] == \"system\"\n",
    "    ]\n",
    "\n",
    "    # Convert the rest of messages to the format expected by the Bedrock Converse API\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": message[\"role\"],\n",
    "            \"content\": (\n",
    "                message[\"content\"]\n",
    "                if isinstance(message[\"content\"], list)\n",
    "                else [{\"text\": message[\"content\"]}]\n",
    "            ),\n",
    "        }\n",
    "        for message in messages\n",
    "        if message[\"role\"] != \"system\"  # Add this condition\n",
    "    ]\n",
    "\n",
    "    # 2. model call with error handling\n",
    "    try:\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=modelId,\n",
    "            messages=messages,\n",
    "            system=system_prompts,\n",
    "            inferenceConfig=inferenceConfig,\n",
    "            additionalModelRequestFields=additionalModelRequestFields,\n",
    "            **kwargs,\n",
    "        )\n",
    "    except (ClientError, Exception) as e:\n",
    "        error_message = f\"ERROR: Can't invoke '{modelId}'. Reason: {e}\"\n",
    "        langfuse_context.update_current_observation(\n",
    "            level=\"ERROR\", status_message=error_message\n",
    "        )\n",
    "        print(error_message)\n",
    "        return\n",
    "\n",
    "    # 3. extract response metadata\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    langfuse_context.update_current_observation(\n",
    "        output=response_text,\n",
    "        usage={\n",
    "            \"input\": response[\"usage\"][\"inputTokens\"],\n",
    "            \"output\": response[\"usage\"][\"outputTokens\"],\n",
    "            \"total\": response[\"usage\"][\"totalTokens\"],\n",
    "        },\n",
    "        metadata={\n",
    "            \"ResponseMetadata\": response[\"ResponseMetadata\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: LiteLLM Proxy\n",
    "When using / evaluating multiple model providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from langfuse.client import PromptClient\n",
    "from langfuse.decorators import langfuse_context, observe\n",
    "\n",
    "import litellm\n",
    "import litellm.types\n",
    "import litellm.types.utils\n",
    "\n",
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "\n",
    "# set callbacks\n",
    "litellm.success_callback = [\"langfuse\"]\n",
    "litellm.failure_callback = [\"langfuse\"]\n",
    "\n",
    "\n",
    "@observe(name=\"example_function\")\n",
    "def fn(\n",
    "    messages: List[Dict[str, str]],\n",
    "    prompt: Optional[PromptClient] = None,\n",
    "    generation_id: Optional[str] = None,\n",
    ") -> str | None:\n",
    "\n",
    "    metadata = {\n",
    "        \"generation_name\": \"test-generation\",  # set langfuse Generation Name\n",
    "        \"existing_trace_id\": langfuse_context.get_current_trace_id(),  # link to parent trace \n",
    "        # GitHub issue for nested traces: https://github.com/langfuse/langfuse/issues/2238\n",
    "    }\n",
    "\n",
    "    if generation_id:\n",
    "        metadata[\"generation_id\"] = generation_id  # override langfuse Generation ID\n",
    "    if prompt:\n",
    "        metadata[\"prompt\"] = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=\"bedrock/anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        messages=messages,\n",
    "        metadata=metadata,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@observe(name=\"example_streaming_function\")\n",
    "def streaming_fn(\n",
    "    messages: List[Dict[str, str]],\n",
    "    prompt: Optional[PromptClient] = None,\n",
    "    generation_id: Optional[str] = None,\n",
    ") -> litellm.utils.CustomStreamWrapper:\n",
    "\n",
    "    metadata = {\n",
    "        \"generation_name\": \"test-generation\",  # set langfuse Generation name\n",
    "        \"existing_trace_id\": langfuse_context.get_current_trace_id(),  # link to parent trace\n",
    "    }\n",
    "\n",
    "    if generation_id:\n",
    "        metadata[\"generation_id\"] = generation_id  # override langfuse Generation ID\n",
    "    if prompt:\n",
    "        metadata[\"prompt\"] = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=\"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        metadata=metadata,\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "@observe(name=\"example_tool_use_function\")\n",
    "def tool_use_fn(\n",
    "    messages: List[Dict[str, str]],\n",
    "    tools: List[Dict[str, str]],\n",
    "    tool_choice: str = \"auto\",\n",
    "    prompt: Optional[PromptClient] = None,\n",
    "    generation_id: Optional[str] = None,\n",
    ") -> List[litellm.types.utils.ChatCompletionMessageToolCall]:\n",
    "\n",
    "    metadata = {\n",
    "        \"generation_name\": \"test-generation\",  # set langfuse Generation name\n",
    "        \"existing_trace_id\": langfuse_context.get_current_trace_id(),  # link to parent trace\n",
    "    }\n",
    "\n",
    "    if generation_id:\n",
    "        metadata[\"generation_id\"] = generation_id  # override langfuse Generation ID\n",
    "    if prompt:\n",
    "        metadata[\"prompt\"] = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=\"bedrock/anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=tool_choice,\n",
    "        metadata=metadata,\n",
    "    )\n",
    "    return response.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Application Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"retrieve_context\")\n",
    "def retrieve_context(city: str) -> str:\n",
    "    \"\"\"Dummy function to retrieve context for the given city.\"\"\"\n",
    "    context = \"\"\"\\\n",
    "21st November 2024\n",
    "Sydney: 24 degrees celcius.\n",
    "New York: 13 degrees celcius.\n",
    "Tokyo: 11 degrees celcius.\"\"\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "@observe(name=\"example_rag\")\n",
    "def call_rag_api(\n",
    "    query: str,\n",
    "    user_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    ") -> Tuple[str]:\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        tags=[\"dev\"],\n",
    "    )\n",
    "\n",
    "    retrieved_context = retrieve_context(query)\n",
    "    # without langfuse prompt manager\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": f\"Context: {retrieved_context}\\nBased on the context above, answer the following question:\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "        {\"content\": query, \"role\": \"user\"},\n",
    "    ]\n",
    "\n",
    "    # with langfuse prompt manager\n",
    "    # qa_with_context_prompt = langfuse.get_prompt(\"qa-with-context\", version=1)\n",
    "    # messages = qa_with_context_prompt.compile(\n",
    "    #     retrieved_context=retrieved_context,\n",
    "    #     query=query,\n",
    "    # )\n",
    "\n",
    "    trace_id=langfuse_context.get_current_trace_id()\n",
    "    generation_id = uuid.uuid4().hex\n",
    "\n",
    "    return fn(\n",
    "        messages, \n",
    "        # prompt=qa_with_context_prompt, # uncomment to link the prompt\n",
    "        # if using LiteLLM functions, pass it down to LiteLLM completion\n",
    "        # generation_id=generation_id, \n",
    "        # if not using LiteLLM, auto-overrides id for functions wrapped with @observe\n",
    "        langfuse_observation_id=generation_id, \n",
    "    ), trace_id, generation_id # return id for async scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(call_rag_api(query=\"What is the temperature in Sydney?\", user_id=\"tenant1-user1\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to create a chat prompt\n",
    "langfuse.create_prompt(\n",
    "    name=\"qa-with-context\",\n",
    "    type=\"chat\",\n",
    "    prompt=[\n",
    "      { \"role\": \"system\", \"content\": f\"Context: {{retrieved_context}}\\nBased on the context above, answer the following question:\" },\n",
    "      { \"role\": \"user\", \"content\": \"{{query}}\" },\n",
    "    ],\n",
    "    config={\n",
    "        \"model\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        \"temperature\": 0.1,\n",
    "    },  # optionally, add configs (e.g. model parameters or model tools) or tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_context_prompt = langfuse.get_prompt(\"qa-with-context\", version=1)\n",
    "messages = qa_with_context_prompt.compile(\n",
    "    retrieved_context=\"<context>\",\n",
    "    query=\"<query>\",\n",
    ")\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring from backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "output, trace_id, generation_id = call_rag_api(query=\"What is the temperature in Sydney?\", user_id=\"tenant1-user1\")\n",
    "\n",
    "# Score the trace from outside the trace context using the low-level SDK\n",
    "# auto evals, score against both observation and trace\n",
    "langfuse.score(\n",
    "    trace_id=trace_id,\n",
    "    observation_id=generation_id,\n",
    "    name=\"accuracy\",\n",
    "    value=random.uniform(0, 1),\n",
    ")\n",
    "\n",
    "# user feedback\n",
    "langfuse.score(\n",
    "    trace_id=trace_id,\n",
    "    name=\"like\",\n",
    "    data_type=\"BOOLEAN\",\n",
    "    value=True,\n",
    "    comment=\"I like how detailed the notes are\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring from frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web SDK example for scoring:\n",
    "* https://langfuse.com/docs/scores/user-feedback#example-using-langfuseweb\n",
    "* https://langfuse.com/docs/sdk/typescript/guide-web\n",
    "\n",
    "```javascript\n",
    "import { LangfuseWeb } from \"langfuse\";\n",
    " \n",
    "export function UserFeedbackComponent(props: { traceId: string }) {\n",
    "  const langfuseWeb = new LangfuseWeb({\n",
    "    publicKey: env.NEXT_PUBLIC_LANGFUSE_PUBLIC_KEY,\n",
    "  });\n",
    " \n",
    "  const handleUserFeedback = async (value: number) =>\n",
    "    await langfuseWeb.score({\n",
    "      traceId: props.traceId,\n",
    "      name: \"user_feedback\",\n",
    "      value,\n",
    "    });\n",
    " \n",
    "  return (\n",
    "    <div>\n",
    "      <button onClick={() => handleUserFeedback(1)}>👍</button>\n",
    "      <button onClick={() => handleUserFeedback(0)}>👎</button>\n",
    "    </div>\n",
    "  );\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run the following cell **ONCE** to create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"city_temperature\"\n",
    "\n",
    "# Uncomment the following code to create a dataset and upload items to it\n",
    "# langfuse.create_dataset(name=dataset_name)\n",
    "\n",
    "# context = retrieve_context(\"What's the temperature?\")\n",
    "# # example items, could also be json instead of strings\n",
    "# local_items = [\n",
    "#     {\"input\": {\"context\": context, \"city\": \"Sydney\"}, \"expected_output\": \"24 degrees celcius\"},\n",
    "#     {\"input\": {\"context\": context, \"city\": \"New York\"}, \"expected_output\": \"13 degrees celcius\"},\n",
    "#     {\"input\": {\"context\": context, \"city\": \"Tokyo\"}, \"expected_output\": \"11 degrees celcius\"},\n",
    "# ]\n",
    "\n",
    "# # Upload to Langfuse\n",
    "# for item in local_items:\n",
    "#   langfuse.create_dataset_item(\n",
    "#       dataset_name=dataset_name,\n",
    "#       # any python object or value\n",
    "#       input=item[\"input\"],\n",
    "#       # any python object or value, optional\n",
    "#       expected_output=item[\"expected_output\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from langfuse.model import DatasetStatus\n",
    "\n",
    "def custom_evaluate(context, query, expected_output, output) -> Tuple[float, str]:\n",
    "    # TODO: define any custom evaluation logic here\n",
    "    # For example, rule-based, LLM-as-judge\n",
    "    return random.uniform(0, 1), \"This is a dummy LLM evaluation\"\n",
    "\n",
    "def run_experiment(run_name: str, user_prompt: str):\n",
    "    dataset = langfuse.get_dataset(dataset_name)\n",
    "\n",
    "    for item in dataset.items:\n",
    "        with item.observe(run_name=run_name) as trace_id:\n",
    "            if item.status is not DatasetStatus.ACTIVE:\n",
    "                print(f\"Skipping {item.id} of status {item.status}\")\n",
    "                continue\n",
    "\n",
    "            print(item.input)\n",
    "            context = item.input[\"context\"]\n",
    "            city = item.input[\"city\"]\n",
    "            query = user_prompt.format(city=city)\n",
    "            expected_output = item.expected_output\n",
    "\n",
    "            output, _, _ = call_rag_api(query=query, user_id=\"evals\")\n",
    "\n",
    "            # evaluation logic\n",
    "            score, comment = custom_evaluate(context, query, expected_output, output)\n",
    "\n",
    "            # # surface the score and comment at trace level\n",
    "            langfuse.score(\n",
    "                trace_id=trace_id,\n",
    "                name=\"accuracy\",\n",
    "                data_type=\"NUMERIC\",\n",
    "                value=score,\n",
    "                comment=comment\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langfuse.decorators import langfuse_context\n",
    " \n",
    "run_experiment(\n",
    "    run_name=f\"generic_ask_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
    "    user_prompt=\"What is the temperature in {city}?\"\n",
    ")\n",
    "run_experiment(\n",
    "    run_name=f\"precise_ask_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
    "    user_prompt=\"What is the temperature in {city}? Respond with the temperature only.\"\n",
    ")\n",
    "\n",
    "# Assert that all events were sent to the Langfuse API\n",
    "langfuse_context.flush()\n",
    "langfuse.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
