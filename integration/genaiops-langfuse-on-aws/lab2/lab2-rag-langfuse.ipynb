{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Retrieval-Augmented Generation (RAG) pipelines with Ragas and Langfuse\n",
    "\n",
    "In this notebook we'll explore ways to evaluate the quality of Retrieval-Augmented Generation (RAG) pipelines with the opensource tools like [RAGAS](https://docs.ragas.io/en/v0.1.21/index.html) and leverage the features in [Langfuse](https://langfuse.com/) to manage and trace the RAG pipelines with traces and spans. We will use the Bedrock knowledge base created in previous lab and the RAG batch generation results to show offline evaluation and scoring. Both at solution development time with open-source tools like Ragas, and at run-time with checks built in to Bedrock itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "#### Additional permissions for Amazon OpenSearch\n",
    "\n",
    "To complete the manual Bedrock Knowledge setup steps in this notebook, your **AWS Console user/role** will need:\n",
    "\n",
    "- [Permissions to work with Amazon OpenSearch vector collections](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vector-search.html)\n",
    "- Permission to **create IAM roles** and attach policies to them, including: `iam:AttachRolePolicy`, `iam:CreateRole`, `iam:DetachRolePolicy`, `iam:GetRole`, `iam:PassRole`, `iam:CreatePolicy`, `iam:CreatePolicyVersion`, and `iam:DeletePolicyVersion`.\n",
    "\n",
    "> ℹ️ **Note:** In testing, we saw `NetworkError` issues when attempting to create Bedrock KBs using only the above-linked `aoss` policy statements. This was resolved by granting `aoss:*` on `*` instead, but you should consider reducing these permissions before using in production environments.\n",
    "\n",
    "If you're in an instructor-led workshop using temporary accounts provided by AWS, this setup should already have been completed for you. If not, refer to the [AWS Console for Identity and Access Management (IAM)](https://console.aws.amazon.com/iam/home?#/home) to grant permissions to your user or role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets ragas llama_index python-dotenv langchain-aws boto3 --upgrade\n",
    "%pip install langfuse==2.54.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to self-hosted or cloud Langfuse environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment variables\n",
    "import os\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-97eba3c3-2e25-4ccc-9f7e-a7c90efd385c\" # Your Langfuse project secret key\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-6e1b61bd-9f28-4844-9f6c-0c11c996644d\" # Your Langfuse project public key\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"http://langfu-loadb-step7dkkf764-813580692.us-west-2.elb.amazonaws.com\" # Region-specific Langfuse domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Langfuse documentation for more details: https://langfuse.com/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock Converse API\n",
    "Next, let's import the libraries that'll be used in the rest of the notebook - and set some configurations that we'll use later:\n",
    "\n",
    "- An Amazon S3 bucket_name is required to store our document corpus. \n",
    "\n",
    "- A folder prefix under the bucket where artifacts will be stored.\n",
    "\n",
    "- Setup Bedrock converse API that will be used to access the Foundation Models (FMs) on Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from langfuse.client import PromptClient\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # General Python SDK for AWS (including Bedrock)\n",
    "import pandas as pd  # For working with tabular data\n",
    "from tqdm.notebook import tqdm  # Progress bars\n",
    "\n",
    "botosess = boto3.Session(region_name=\"us-west-2\")\n",
    "region = botosess.region_name\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "bucket_name = f'eval-{account_id}-{region}'\n",
    "s3_prefix = \"bedrock-rag-eval\"\n",
    "\n",
    "# check if s3 bucket exists or not, if not, create bucket\n",
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket {bucket_name} exists\")\n",
    "except ClientError:\n",
    "    print(f\"Creating bucket {bucket_name}\")\n",
    "    s3.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={'LocationConstraint': region}\n",
    "    )\n",
    "\n",
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to access Bedrock configuration\n",
    "bedrock = boto3.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    " \n",
    "# used to invoke the Bedrock Converse API\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    "\n",
    "bedrock_agent_runtime = boto3.client(\n",
    "    service_name=\"bedrock-agent-runtime\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# Check which models are available in your account\n",
    "models = bedrock.list_inference_profiles()\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "  print(model[\"inferenceProfileName\"] + \" - \" + model[\"inferenceProfileId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the knowledge base\n",
    "\n",
    "> ⚠️ ***Watch out:** This section includes steps you'll need to take manually, not just running the code cells!*\n",
    "\n",
    "First, we'll need to upload the sample documents to Amazon S3 - for which you can just run the code cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHERE IS THE STEP TO CREATE THIS BUCKET????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_s3uri = f\"s3://{bucket_name}/{s3_prefix}/corpus\"\n",
    "\n",
    "print(f\"Syncing corpus to:\\n{corpus_s3uri}/\")\n",
    "\n",
    "!aws s3 sync --quiet ./datasets/corpus {corpus_s3uri}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to set up the actual Bedrock Knowledge Base for testing will be **manually through the AWS Console**:\n",
    "\n",
    "▶️ First, **open** the [AWS Console for Amazon Bedrock](https://console.aws.amazon.com/bedrock/home?#/knowledge-bases) and select *Orchestration > Knowledge bases* from the left sidebar menu, as shown in the screenshot below:\n",
    "\n",
    "> ⚠️ **Check** you're working in the correct *AWS Region* in the top right corner of the UI\n",
    "\n",
    "![](images/bedrock-kbs/01-bedrock-kb-console.png \"Screenshot of AWS Console for Amazon Bedrock Knowledge Bases, showing 'Create knowledge base' action button\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Click** the *Create knowledge base* button and sleect *Knowledge Base with vector *. In the screen that opens:\n",
    "\n",
    "- For **knowledge base name**, enter `example-squad-kb`\n",
    "- For **knowledge base description**, you can provide (something like) `Demo knowledge base for question answering evaluation`\n",
    "- Leave the other settings as default (allow creating a new execution role, and no tags)\n",
    "- Please chose Amazon S3 as the data source (default)\n",
    "\n",
    "Your configuration should be as shown below:\n",
    "\n",
    "![](images/bedrock-kbs/02a-create-kb-basics.png \"Screenshot of step 1 in Bedrock Knowledge Base creation workflow: with KB name, description, (create new) execution role, and (empty) tags configured. At the end of the form, a 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ In the **Next** screen, you'll configure the S3 data source:\n",
    "\n",
    "Please the data source name as it is and then select the bucket and prefix per you created in the previous step and use Amazon Bedrock default parser. The bucket name in the screenshot is just an example.\n",
    "\n",
    "![](images/bedrock-kbs/02b-create-kb-data-source.png \"Screenshot of Knowledge Base vector index settings including Cohere Embed Multilingual embedding model, and quick-create vector store. 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ In the **Next** screen, you'll configure the vector index:\n",
    "\n",
    "- For **embeddings model**, select `Cohere Embed Multilingual`\n",
    "\n",
    "> ⚠️ **Check** in the [Amazon Bedrock Model Access console](https://console.aws.amazon.com/bedrock/home?#/modelaccess) that you've enabled access to this model in the current region.\n",
    ">\n",
    "> If needed, you should be able to select an alternative embedding model instead... But we haven't tested all options for this walkthrough.\n",
    "\n",
    "- For **Vector database**, select `Quick create a new vector store`\n",
    "\n",
    "You can find more information from this screen or the [Amazon Bedrock Developer Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-setup.html) about the different vector stores Bedrock Knowledge Bases support. This default option will create a new [Amazon OpenSearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html) cluster\n",
    "\n",
    "Leave other settings at their defaults as shown below, and you should be ready to proceed:\n",
    "\n",
    "![](images/bedrock-kbs/02c-create-kb-index.png \"Screenshot of Knowledge Base vector index settings including Cohere Embed Multilingual embedding model, and quick-create vector store. 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ Click **Next** to review your configuration, and then **Create knowledge base** to complete the process.\n",
    "\n",
    "> ⏰ It might take **a few minutes** for the creation to complete. A progress indicator banner should be visible if you scroll up. Alternatively in a separate tab, you could check the [Amazon OpenSearch Serverless Collections console](https://console.aws.amazon.com/aos/home?#opensearch/collections) - where you should see the underlying vector collection being created.\n",
    "\n",
    "Once your Knowledge Base is completed successfully, you'll be directed to the its detail screen as shown below:\n",
    "\n",
    "![](images/bedrock-kbs/03-kb-detail-page.png \"Detail screen for the created Amazon Bedrock Knowledge Base, showing creation success banner. Includes sections 'Knowledge base overview' (containing the KB ID, name, and other details); 'Tags' (empty); 'Data source' (one Amazon S3 data source listed); 'Embeddings model' (Cohere Embed); and an interactive 'Test knowledge base' chat sidebar on the right with a warning that some data sources have not been synced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the alert box shown ahead, your new knowledge base will not yet contain your documents until we **sync** the data source:\n",
    "\n",
    "▶️ **Select** your S3 data source using the radio button to the left of it's name in the data sources list, and **click the Sync button** above to start the sync.\n",
    "\n",
    "The sync should only take a few seconds, after which your data source's *Status* will return to `Available`\n",
    "\n",
    "![](images/bedrock-kbs/04a-kb-data-source-after-sync.png \"Screenshot of KB 'data source' section after running sync, with the data source selected and status showing as 'available'\")\n",
    "\n",
    "With the sync completed, your Knowledge Base should be ready to use.\n",
    "\n",
    "Optionally, you can click through to your data source name to check the sync `Added` the 20 files as expected:\n",
    "\n",
    "![](images/bedrock-kbs/04b-kb-data-sync-details.png \"Data source details screen showing sync completed successfully with 20 files detected and added to the index, and 0 files failed\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test out your Knowledge Base\n",
    "\n",
    "Before we discuss evaluation at scale, let's run a couple of test queries to check the KB is working properly.Let's back to the main page of the knowledge base\n",
    "\n",
    "![](images/bedrock-kbs/04c-kb-main-page.png \"Screenshot of the main page of the knowledge base\")\n",
    "\n",
    "You can find the knowledge base id is `Z746ERZP5X` (please check your own knowledge base id) on the top of the page.\n",
    "\n",
    "▶️ **Replace** the below placeholder with your knowledge base's unique ID, and run the cells below to continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_id = \"TO FILL\"  # Something like \"55GUAMQYUT\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ID identified, you can use the Bedrock runtime RetrieveAndGenerate API (see corresponding boto3 doc page for Python) to query your knowledge base.\n",
    "\n",
    "As in the manual example, you'll also need to select which text generation model to use - which we've pre-populated below for Amazon Nova Pro model. Please replace the account id with your own account id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_resp = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\"text\": \"In what country is Normandy located?\"},\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            \"knowledgeBaseId\": knowledge_base_id,\n",
    "            \"modelArn\": f\"arn:aws:bedrock:us-west-2:{account_id}:inference-profile/us.amazon.nova-pro-v1:0\",\n",
    "        },\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "    },\n",
    "    # Optional session ID can help improve results for follow-up questions:\n",
    "    # sessionId='string'\n",
    ")\n",
    "\n",
    "print(\"Plain text response:\")\n",
    "print(\"--------------------\")\n",
    "print(rag_resp[\"output\"][\"text\"], end=\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Full API output:\")\n",
    "print(\"----------------\")\n",
    "rag_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_resp = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\"text\": \"In what country is Normandy located?\"},\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            \"knowledgeBaseId\": knowledge_base_id,\n",
    "            \"modelArn\": f\"arn:aws:bedrock:us-west-2:891377141484:inference-profile/us.amazon.nova-pro-v1:0\",\n",
    "        },\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "    },\n",
    "    # Optional session ID can help improve results for follow-up questions:\n",
    "    # sessionId='string'\n",
    ")\n",
    "\n",
    "print(\"Plain text response:\")\n",
    "print(\"--------------------\")\n",
    "print(rag_resp[\"output\"][\"text\"], end=\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Full API output:\")\n",
    "print(\"----------------\")\n",
    "rag_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the full API response from the above cell, the `RetrieveAndGenerate` action provides:\n",
    "\n",
    "- The final text answer\n",
    "- The `retrievedReferences` from the search engine\n",
    "- Specific `citations` localizing which references should be cited by different parts of the text answer\n",
    "\n",
    "\n",
    "It's also possible to run **only the retrieval** through the API, and skip the generative answer synthesis step - as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_resp = bedrock_agent_runtime.retrieve(\n",
    "    knowledgeBaseId=knowledge_base_id,\n",
    "    retrievalQuery={\"text\": \"In what country is Normandy located?\"},\n",
    ")\n",
    "print(json.dumps(retrieve_resp[\"retrievalResults\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG evaluation example with dummy dataset\n",
    "\n",
    "#### Load Dataset\n",
    "\n",
    "For this example, we are going to use a dataset that has already been prepared by querying a RAG system and gathering its outputs. See below for instruction on how to fetch your production data from Langfuse.\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "- `question`: list[str] - These are the questions your RAG pipeline will be evaluated on.\n",
    "\n",
    "- `contexts`: list[list[str]] - The contexts which were passed into the LLM to answer the question.\n",
    "\n",
    "- `answer`: list[str] - The answer generated from the RAG pipeline and given to the user.\n",
    "\n",
    "- `ground_truths`: list[list[str]] - The ground truth answer to the questions. However, this can be ignored for online evaluations since we will not have access to ground-truth data in our case.\n",
    "\n",
    "For the details of this dataset, please refer to [Exploding Gradients Dataset](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")['baseline']\n",
    "fiqa_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The eval metrics\n",
    "For going to measure the following aspects of a RAG system. These metric are from the Ragas library:\n",
    "\n",
    "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/): This measures the factual consistency of the generated answer against the given context.\n",
    "- [Response relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/): The ResponseRelevancy metric measures how relevant a response is to the user input.\n",
    "- [Context precision](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/): Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked high. Ideally all the relevant chunks must appear at the top ranks.\n",
    "\n",
    "Checkout the [RAGAS documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/) to know more about these metrics and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    "    LLMContextPrecisionWithoutReference,\n",
    ")\n",
    " \n",
    "# metrics you chose\n",
    "metrics = [\n",
    "    Faithfulness(),\n",
    "    ResponseRelevancy(),\n",
    "    LLMContextPrecisionWithoutReference(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to initialize the metrics with LLMs and Embeddings of your choice. In this example we are going to use the Bedrock Nova LLMs and Titan embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\n",
    "\n",
    "\n",
    "# util function to init Ragas Metrics\n",
    "def init_ragas_metrics(metrics, llm, embedding):\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, MetricWithLLM):\n",
    "            print(metric.name + \" llm\")\n",
    "            metric.llm = llm\n",
    "        if isinstance(metric, MetricWithEmbeddings):\n",
    "            print(metric.name + \" embedding\")\n",
    "            metric.embeddings = embedding\n",
    "        run_config = RunConfig()\n",
    "        metric.init(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "config = {\n",
    "    \"region_name\": \"us-west-2\",  # E.g. \"us-east-1\"\n",
    "    \"llm\": \"us.amazon.nova-pro-v1:0\",  # E.g you can also use the claude models \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "    \"embeddings\": \"cohere.embed-english-v3\",  # E.g or \"amazon.titan-embed-text-v2:0\"\n",
    "    \"temperature\": 0.4,\n",
    "}\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatBedrockConverse(\n",
    "    region_name=config[\"region_name\"],\n",
    "    base_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n",
    "    model=config[\"llm\"],\n",
    "    temperature=config[\"temperature\"],\n",
    "))\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(BedrockEmbeddings(\n",
    "    region_name=config[\"region_name\"],\n",
    "    model_id=config[\"embeddings\"],\n",
    "))\n",
    "\n",
    "\n",
    "init_ragas_metrics(\n",
    "    metrics,\n",
    "    llm=evaluator_llm,\n",
    "    embedding=evaluator_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace RAGAS eval results on Langfuse\n",
    "\n",
    "You can use model-based evaluation with Ragas in 2 ways:\n",
    "1. Score each Trace: This means you will run the evaluations for each trace item. This gives you much better idea of how each call made to your RAG pipelines is performing, but please be mindful of the cost.\n",
    "\n",
    "2. Score as Batch: In this method we will take a random sample of traces on a periodic basis and score them. This brings down the cost and gives you a rough estimate the performance of your app but may miss out on important samples.\n",
    "\n",
    "In this example, we will demonstrate both the solutions using both prebuilt dataset and a live RAG pipeline with Bedrock Knowlegebase.\n",
    "\n",
    "#### Score with Trace\n",
    "\n",
    "Lets take a small example of a single trace and see how you can score that with Ragas. We first define a utility function to score your trace with the metrics you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    " \n",
    "async def score_with_ragas(query, chunks, answer):\n",
    "    scores = {}\n",
    "    for metric in metrics:\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=query,\n",
    "            retrieved_contexts=chunks,\n",
    "            response=answer,\n",
    "        )\n",
    "        print(f\"calculating {metric.name}\")\n",
    "        scores[metric.name] = await metric.single_turn_ascore(sample)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You compute the score with each request. Below we will go through a dummy application that does the following steps:\n",
    "\n",
    "- gets a question from the user\n",
    "\n",
    "- fetch context from the database or vector store that can be used to answer the question from the user\n",
    "\n",
    "- pass the question and the contexts to the LLM to generate the answer\n",
    "\n",
    "All these step are logged as spans in a single trace in langfuse. You can read more about the traces and spans, which are the low-level SDK, from the [langfuse documentation](https://langfuse.com/docs/sdk/python/low-level-sdk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new trace when you get a question\n",
    "row = fiqa_eval[0]\n",
    "question = row['question']\n",
    "trace = langfuse.trace(name = \"rag\")\n",
    " \n",
    "# retrieve the relevant chunks\n",
    "# chunks = get_similar_chunks(question)\n",
    "contexts = row['contexts']\n",
    "# pass it as span\n",
    "trace.span(\n",
    "    name = \"retrieval\", input={'question': question}, output={'contexts': contexts}\n",
    ")\n",
    " \n",
    "# use llm to generate a answer with the chunks\n",
    "# answer = get_response_from_llm(question, chunks)\n",
    "answer = row['answer']\n",
    "trace.span(\n",
    "    name = \"generation\", input={'question': question, 'contexts': contexts}, output={'answer': answer}\n",
    ")\n",
    "\n",
    "# compute scores for the question, context, answer tuple\n",
    "ragas_scores = await score_with_ragas(question, contexts, answer)\n",
    "ragas_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see this is traced in langfuse but with no score attached, check it in the Langfuse ui.\n",
    "![](images/bedrock-kbs/04d-langfuse-single-eval-trace-no-score.png)\n",
    "You can then attach the scores to the trace by running the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the scores\n",
    "for m in metrics:\n",
    "    trace.score(name=m.name, value=ragas_scores[m.name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the score is attached\n",
    "![](images/bedrock-kbs/04e-langfuse-single-eval-trace-score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Bedrock Knowledge Base RAG pipelines using RAGAS and Langfuse\n",
    "We have already setup the Bedrock Knowledge Base in the first section, we will now **evaluate** the quality of its results against a test dataset - to help us **optimize** the configuration for high quality and low cost.\n",
    "\n",
    "First, let's load the sample dataset of questions, reference answers, and their source documents (to find more of how to prepare this dataset, please see more details in [this github](https://github.com/aws-samples/llm-evaluation-methodology/blob/main/datasets/Prepare-SQuAD.ipynb)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_json(\"datasets/qa.manifest.jsonl\", lines=True)\n",
    "dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records in this dataset include:\n",
    "\n",
    "- (`doc`) The full text of the source document for this example\n",
    "- (`doc_id`) A unique identifier for the source document\n",
    "- (`question`) The user question to be asked\n",
    "- (`question_id`) A unique identifier for the question\n",
    "- (`answers`) A list of (possibly multiple) reference 'correct' answers, supported by the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the knowledge base against our test set\n",
    "\n",
    "As shown in [Ragas' API Reference](https://docs.ragas.io/en/latest/references/evaluation.html), records in Ragas evaluation datasets typically include:\n",
    "\n",
    "- The `question` that was asked\n",
    "- The `answer` the system generated\n",
    "- The actual text `contexts` the answer was based on (i.e. snippets of document text retrieved by the search engine)\n",
    "- The `ground_truth` answer(s)\n",
    "\n",
    "Here we will integrate [Langfuse Tracking](https://langfuse.com/docs/tracing) into the RAG pipeline with the Langfuse Python SDK using the `@observe()` decorator.\n",
    "\n",
    "We can run our example questions through the Bedrock KB RAG as shown below, to fetch the outputs ready to calculate metrics:\n",
    "Please fill in the *account id* in the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import tqdm explicitly\n",
    "from tqdm.notebook import tqdm\n",
    "# Then import ThreadPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "from datasets import Dataset  # For use with Ragas\n",
    "\n",
    "@observe(as_type=\"generation\", name=\"retrieve_generate_kb\")\n",
    "def retrieve_and_generate(\n",
    "    question: str,\n",
    "    kb_id: str,\n",
    "    generate_model_arn: str = f\"arn:aws:bedrock:us-west-2:{account_id}:inference-profile/us.amazon.nova-pro-v1:0\",\n",
    "    **kwargs,\n",
    "):\n",
    "    rag_resp = bedrock_agent_runtime.retrieve_and_generate(\n",
    "        input={\"text\": question},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": generate_model_arn,\n",
    "            },\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "        },\n",
    "    )\n",
    "    answer = rag_resp[\"output\"][\"text\"]\n",
    "    # Fetch flat list of references from the nested citations->retrievedReferences:\n",
    "    all_refs = [r for cite in rag_resp[\"citations\"] for r in cite[\"retrievedReferences\"]]\n",
    "    contexts = [r[\"content\"][\"text\"] for r in all_refs]\n",
    "    ref_s3uris = [r[\"location\"][\"s3Location\"][\"uri\"] for r in all_refs]\n",
    "    # Map e.g. 's3://.../doc_id.txt' to 'doc_id':\n",
    "    ref_ids = [uri.rpartition(\"/\")[2].rpartition(\".\")[0] for uri in ref_s3uris]\n",
    "    langfuse_context.update_current_observation(\n",
    "        input={\"question\":question,\n",
    "               \"contexts\": contexts},\n",
    "        output=answer,\n",
    "        model=generate_model_arn,\n",
    "        tags=[\"dev\"],\n",
    "        metadata=kwargs\n",
    "    )\n",
    "    trace_id=langfuse_context.get_current_trace_id()\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_doc_ids\": ref_ids,\n",
    "        \"retrieved_doc_texts\": contexts,\n",
    "        \"trace_id\": trace_id\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_generated_outputs = [\n",
    "    retrieve_and_generate(question=rec.question, \n",
    "                          kb_id=knowledge_base_id, \n",
    "                          kwargs={\"database\":\"Bedrock_kb\",\n",
    "                                    \"kb_id\": knowledge_base_id}\n",
    "                    )\n",
    "    for _, rec in dataset_df.iterrows()\n",
    "]\n",
    "rag_generated_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_df = pd.DataFrame(rag_generated_outputs, columns=[\"answer\", \"retrieved_doc_ids\", \"retrieved_doc_texts\", \"trace_id\"])\n",
    "# Combine & clarify the column names for a nice tabular representation:\n",
    "results_df = pd.concat((dataset_df, outputs_df), axis=1).rename(\n",
    "    columns={\n",
    "        \"answer\": \"model_answer\",\n",
    "        \"answers\": \"gt_answers\",\n",
    "        \"doc_id\": \"gt_doc_id\",\n",
    "        \"doc\": \"gt_doc_text\",\n",
    "    }\n",
    ")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to re-initiate the ragas metrics\n",
    "init_ragas_metrics(\n",
    "    metrics,\n",
    "    llm=evaluator_llm,\n",
    "    embedding=evaluator_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe, langfuse_context\n",
    "from typing import Optional\n",
    "from asyncio import run\n",
    "@observe(as_type=\"generation\", name=\"br_kb_rag\")\n",
    "def rag_pipeline(question,\n",
    "                 user_id: Optional[str] = None,\n",
    "                 session_id: Optional[str] = None,\n",
    "                 kb_id: Optional[str] = None,\n",
    "                 metrics: Optional[Any] = None\n",
    "    ):\n",
    "\n",
    "    generated_answer = retrieve_and_generate(question=question, \n",
    "                                             kb_id=kb_id, \n",
    "                                             kwargs={\"database\":\"Bedrock_kb\", \n",
    "                                                     \"kb_id\": knowledge_base_id}\n",
    "                                            )\n",
    "    contexts= generated_answer[\"retrieved_doc_texts\"]\n",
    "    answer= generated_answer[\"answer\"]\n",
    "    trace_id= generated_answer[\"trace_id\"]\n",
    "\n",
    "    score = run(score_with_ragas(question, contexts, answer=answer))\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        tags=[kb_id],\n",
    "    )\n",
    "    for s in score:\n",
    "        langfuse.score(name=s, value=score[s], trace_id=trace_id)\n",
    "    return generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_pipeline(dataset_df.iloc[0][\"question\"], kb_id=knowledge_base_id)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring as batch\n",
    "\n",
    "Scoring each production trace can be time-consuming and costly depending on your application architecture and traffic. In that case, it's better to start off with a batch scoring method. Decide a timespan you want to run the batch process and the number of traces you want to sample from that time slice. Create a dataset and call ragas.evaluate to analyze the result.\n",
    "\n",
    "You can run this periodically to keep track of how the scores are changing across timeslices and figure out if there are any discrepancies.\n",
    "\n",
    "We will evaluate the existing results generated previously by the `retrieve_and_generate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, interaction in results_df.head(10).iterrows():\n",
    "    trace = langfuse.trace(name = \"br_kb_rag_span\")\n",
    "    trace.span(\n",
    "        name = \"retrieval\",\n",
    "        input={\"question\": interaction[\"question\"]},\n",
    "        output={\"contexts\": interaction[\"retrieved_doc_texts\"]},\n",
    "        metadata={\"comments\":\"offline batch update\"}\n",
    "    )\n",
    "    trace.span(\n",
    "        name = \"generation\",\n",
    "        input={\"question\": interaction[\"question\"], 'contexts': interaction[\"retrieved_doc_texts\"]},\n",
    "        output={\"answer\": interaction[\"model_answer\"]},\n",
    "        metadata={\"comments\":\"offline batch update\"}\n",
    "    )\n",
    "\n",
    "# await that Langfuse SDK has processed all events before trying to retrieve it in the next step\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the results are uploaded to langfuse you can retrieve it as needed with this handy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traces(name=None, limit=None, user_id=None):\n",
    "    all_data = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        response = langfuse.client.trace.list(\n",
    "            name=name, page=page, user_id=user_id\n",
    "        )\n",
    "        if not response.data:\n",
    "            break\n",
    "        page += 1\n",
    "        all_data.extend(response.data)\n",
    "        if len(all_data) > limit:\n",
    "            break\n",
    "\n",
    "    return all_data[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "NUM_TRACES_TO_SAMPLE = 3\n",
    "traces = get_traces(name='br_kb_rag_span', limit=5)\n",
    "traces_sample = sample(traces, NUM_TRACES_TO_SAMPLE)\n",
    "\n",
    "len(traces_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make a batch and score it. Ragas uses huggingface dataset object to build the dataset and run the evaluation. If you run this on your own production data, use the right keys to extract the question, contexts and answer from the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on a sample\n",
    "from random import sample\n",
    "\n",
    "evaluation_batch = {\n",
    "    \"question\": [],\n",
    "    \"contexts\": [],\n",
    "    \"answer\": [],\n",
    "    \"trace_id\": [],\n",
    "}\n",
    "\n",
    "for t in traces_sample:\n",
    "    observations = [langfuse.client.observations.get(o) for o in t.observations]\n",
    "    for o in observations:\n",
    "        if o.name == 'retrieval':\n",
    "            question = o.input['question']\n",
    "            contexts = o.output['contexts']\n",
    "        if o.name=='generation':\n",
    "            answer = o.output['answer']\n",
    "    evaluation_batch['question'].append(question)\n",
    "    evaluation_batch['contexts'].append(contexts)\n",
    "    evaluation_batch['answer'].append(answer)\n",
    "    evaluation_batch['trace_id'].append(t.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ragas evaluate\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy\n",
    "\n",
    "ds = Dataset.from_dict(evaluation_batch)\n",
    "r = evaluate(ds, \n",
    "             llm=evaluator_llm, \n",
    "             embeddings=evaluator_embeddings, \n",
    "             metrics=[Faithfulness(), ResponseRelevancy()]\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it! You can see the scores over a time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push the scores back into Langfuse or use the exported pandas dataframe to run further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = r.to_pandas()\n",
    "\n",
    "# add the langfuse trace_id to the result dataframe\n",
    "df[\"trace_id\"] = ds[\"trace_id\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    for metric_name in [\"faithfulness\", \"answer_relevancy\"]:\n",
    "        langfuse.score(\n",
    "            name=metric_name,\n",
    "            value=row[metric_name],\n",
    "            trace_id=row[\"trace_id\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now go back to the Langfuse console and check the updated scores in the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
