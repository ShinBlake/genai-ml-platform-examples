{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d8b8be2-25d0-4fd8-adb3-57857424ed84",
   "metadata": {},
   "source": [
    "# ラボ 3.1: 外部評価パイプラインを使用して Langfuse トレースを評価する\n",
    "\n",
    "#### 外部評価パイプラインは、次の場合に役立ちます：\n",
    "- トレース評価のタイミングをより細かく制御できる。パイプラインを特定の時間に実行するようにスケジュールしたり、Webhook などのイベントベースのトリガーに応答させたりできる。\n",
    "- Langfuse UI で可能な範囲を超えたカスタム評価が必要な場合、カスタム評価に柔軟性を持たせることができる。\n",
    "- カスタム評価のバージョン管理\n",
    "- 既存の評価フレームワークと事前定義されたメトリクスを使用してデータを評価する機能\n",
    "\n",
    "このノートブックでは、以下の手順で外部評価パイプラインを実装する方法を学習します：\n",
    "1. モデルをテストするための合成データセットを作成する\n",
    "2. Langfuse クライアントを使用して、以前のモデル実行のトレースを収集およびフィルタリングする\n",
    "3. これらのトレースをオフラインで、かつ段階的に評価する\n",
    "4. 既存の Langfuse トレースにスコアを追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699cf5b2",
   "metadata": {},
   "source": [
    "## 前提条件\n",
    "\n",
    "> ℹ️ AWS が提供する一時アカウントを使用してインストラクター主導のワークショップに参加している場合は、**これらの前提条件の手順をスキップできます**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76becf-bfb8-48d5-bc3b-ca6b29413351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AWS ワークショップ環境を使用していない場合は、以下の行のコメントを外して依存関係をインストールしてください\n",
    "# %pip install langfuse datasets ragas python-dotenv langchain-aws boto3 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32f49e",
   "metadata": {},
   "source": [
    "Connect to self-hosted or cloud Langfuse environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0cbc1-7af1-4875-bd2a-0e17ff33cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# すでに VS Code サーバーの .env で環境変数を定義している場合は、以下のセルはスキップしてください。\n",
    "# langfuse 用の環境変数を定義してください。\n",
    "# これらの値は Langfuse で API キーを作成する際に確認することができます。\n",
    "# import os\n",
    "# os.environ[\"LANGFUSE_SECRET_KEY\"] = \"xxxx\" # Langfuse プロジェクトのシークレットキー\n",
    "# os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"xxxx\" # Langfuse プロジェクトのパブリックキー\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"xxx\" # Langfuse ドメイン"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66d076",
   "metadata": {},
   "source": [
    "## 初期化と認証チェック\n",
    "以下のセルを実行して、共通ライブラリとクライアントを初期化してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6967f-1a23-4029-b107-c38e2c74b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "from langfuse.decorators import langfuse_context, observe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5342ee",
   "metadata": {},
   "source": [
    "Amazon Bedrock クライアントを初期化し、アカウントで利用可能なモデルを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9823c3f-bfc3-4507-bc14-42bccda623f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Amazon Bedrock の設定にアクセスするために利用\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-west-2\")\n",
    "\n",
    "# このリージョンで Nova モデルが利用可能か確認\n",
    "models = bedrock.list_inference_profiles()\n",
    "nova_found = False\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "    if (\n",
    "        \"Nova Pro\" in model[\"inferenceProfileName\"]\n",
    "        or \"Nova Lite\" in model[\"inferenceProfileName\"]\n",
    "        or \"Nova Micro\" in model[\"inferenceProfileName\"]\n",
    "    ):\n",
    "        print(\n",
    "            f\"Found Nova model: {model['inferenceProfileName']} - {model['inferenceProfileId']}\"\n",
    "        )\n",
    "        nova_found = True\n",
    "if not nova_found:\n",
    "    raise ValueError(\n",
    "        \"No Nova models found in available models. Please ensure you have access to Nova models.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e58d2",
   "metadata": {},
   "source": [
    "Initialize the Langfuse client and check credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa452fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# langfuse クライアント\n",
    "langfuse = Langfuse()\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse は正しく設定されています\")\n",
    "    print(f\"Langfuse インスタンスへはこちらからアクセスできます: {os.environ['LANGFUSE_HOST']}\")\n",
    "else:\n",
    "    print(\n",
    "        \"認証情報が見つからないか問題があります。.env ファイル内の Langfuse API キーとホストを確認してください。\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b511da7",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Converse API の Langfuse ラッパー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))  # Add parent directory to path\n",
    "from config import MODEL_CONFIG\n",
    "from utils import converse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed397a-fd9c-47b1-8d42-cea1894ef30d",
   "metadata": {},
   "source": [
    "# 合成データの生成\n",
    "\n",
    "このノートブックでは、LLM を活用して、e コマースページで製品に関してアドバイスする際に使用できる製品説明を生成するユースケースを検討します。最初のステップでは、製品のリストを生成し、それぞれの製品に対して Amazon Nova Lite に簡潔な製品説明を生成するよう指示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45d951-8f92-4a0a-8755-bf3b58df7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 製品を生成するプロンプト\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"e コマースウェブサイトで販売されている 50 種類の異なる製品カテゴリーについて、\\\n",
    "        消費者にとって興味深い製品を 1 つずつ生成します。製品名は実際の販売されている製品を反映したものにする必要があります。 \\\n",
    "        50 の製品アイテムをカンマ区切りの値として生成します。製品名以外に追加の単語は生成しないでください。\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Nova Lite モデルの API 呼び出し\n",
    "model_response = converse(messages=messages, **MODEL_CONFIG[\"nova_lite\"])\n",
    "\n",
    "# 生成されたテキストを表示\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b45f91-beb0-42ab-8741-1544027caa95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# モデルが生成した出力を確認\n",
    "products = [item.strip() for item in model_response.split(\",\")]\n",
    "\n",
    "for prd in products:\n",
    "    print(prd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa9f700-bd36-4625-b387-3ae16740a3d7",
   "metadata": {},
   "source": [
    "次に、各製品について Amazon Nova Lite を使用して製品説明を生成し、```@observe()``` デコレータを使用して Langfuse にトレースをキャプチャします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c48c0-9ed1-409b-8b20-b46404b264ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 各製品の製品説明を生成\n",
    "prompt_template = \"あなたはプロダクトマーケターであり、e コマースウェブサイトで \\\n",
    "製品を販売するために使用される詳細な製品説明を生成する必要があります。 \\\n",
    "説明のキャッチーなフレーズは、ソーシャルメディアキャンペーンにも使用されます。 \\\n",
    "製品説明から、お客様は製品が生活にどのように役立つかを理解できるだけでなく、 \\\n",
    "この会社を信頼できることも理解できるべきです。あなたの説明は楽しく魅力的です。 \\\n",
    "あなたの回答は最大 4 文にしてください。\"\n",
    "\n",
    "\n",
    "@observe(name=\"Batch Product Description Generation\")\n",
    "def main():\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"nova-batch-generation-session\",\n",
    "        tags=[\"lab3.1\"],\n",
    "    )\n",
    "\n",
    "    for product in products:\n",
    "        print(f\"Input: {product} の説明文を生成してください。\")\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt_template},\n",
    "            {\"role\": \"user\", \"content\": f\"{product} の説明文を生成してください。\"},\n",
    "        ]\n",
    "        response = converse(\n",
    "            messages, metadata={\"product\": product}, **MODEL_CONFIG[\"nova_lite\"]\n",
    "        )\n",
    "        print(f\"Output: {response} \\n\")\n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63401ad3",
   "metadata": {},
   "source": [
    "これで、langfuse UI のトレースセクションにこれらの製品説明が表示されるはずです。\n",
    "\n",
    "![Traces collected from the LLM generations](./images/product_description_traces.png \"Langfuse Traces\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1d0dd-370e-4053-a90a-c3ddfb095677",
   "metadata": {},
   "source": [
    "このチュートリアルの目標は、モデルベースの評価パイプラインを構築する方法を示すことです。これらのパイプラインは、CI/CD 環境で実行されるか、異なるオーケストレーションされたコンテナサービスで実行されるでしょう。選択する環境に関係なく、常に 3 つの重要なステップが適用されます：\n",
    "\n",
    "1. トレースを取得：アプリケーショントレースを評価環境に取得する\n",
    "2. 評価を実行：任意の評価ロジックを適用する\n",
    "3. 結果を保存：評価の計算に使用した Langfuse トレースに評価を戻して添付する\n",
    "\n",
    "***\n",
    "目標：この評価パイプラインは、過去 24 時間のすべてのトレースに対して実行されます\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0762d57d-ad9d-4157-a055-4f1cd68bee0f",
   "metadata": {},
   "source": [
    "## 1. Fetch the traces\n",
    "\n",
    "The ```fetch_traces()``` function has arguments to filter the traces by tags, timestamps, and beyond. We can also choose the number of samples for pagination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9280ea-d355-40ee-ab59-b91715e16a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "now = datetime.now()\n",
    "last_24_hours = now - timedelta(days=1)\n",
    "\n",
    "traces_batch = langfuse.fetch_traces(\n",
    "    page=1,\n",
    "    limit=1,\n",
    "    tags=\"lab3.1\",\n",
    "    session_id=\"nova-batch-generation-session\",\n",
    "    from_timestamp=last_24_hours,\n",
    "    to_timestamp=datetime.now(),\n",
    ").data\n",
    "\n",
    "print(f\"Trace ID: {traces_batch[0].id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ce965",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = langfuse.fetch_observations(trace_id=traces_batch[0].id, type=\"GENERATION\").data\n",
    "observations[0].output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff8eea-302e-44ee-a4bf-7595c848f872",
   "metadata": {},
   "source": [
    "## 2. Categorical Evaluation using LLM-as-a-judge\n",
    "\n",
    "Evaluation functions should take a trace as input and yield a valid score.\n",
    "When analyzing the outputs of your LLM applications, you may want to evaluate traits that are defined qualitatively such as readability, helpfulness or measures for reducing hallucinations such as completeness.\n",
    "\n",
    "We're building product descriptions and to ensure it resonates with customers, we want to measure readability. For more LLM-as-a-judge definitions, check out the judge based evaluator prompts defined in the [Amazon Bedrock Evaluator Prompts](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-judge-prompt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7fe3a-0ce0-408a-a68b-1b79a11aab29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "template_readability = \"\"\"\n",
    "You are a helpful agent that can assess an LLM response according to the given rubrics.\n",
    "\n",
    "You are given a product description generated by a LLM. Your task is to assess the readability of the LLM response to the question, in other words, how easy it is for a typical reading audience to comprehend the response at a normal reading rate.\n",
    "\n",
    "Please rate the readability of the response based on the following scale:\n",
    "- unreadable: The response contains gibberish or could not be comprehended by any normal audience.\n",
    "- poor readability: The response is comprehensible, but it is full of poor readability factors that make comprehension very challenging.\n",
    "- fair readability: The response is comprehensible, but there is a mix of poor readability and good readability factors, so the average reader would need to spend some time processing the text in order to understand it.\n",
    "- good readability: Very few poor readability factors. Mostly clear, well-structured sentences. Standard vocabulary with clear context for any challenging words. Clear organization with topic sentences and supporting details. The average reader could comprehend by reading through quickly one time.\n",
    "- excellent readability: No poor readability factors. Consistently clear, concise, and varied sentence structures. Simple, widely understood vocabulary. Logical organization with smooth transitions between ideas. The average reader may be able to skim the text and understand all necessary points.\n",
    "\n",
    "Here is the product description that needs to be evaluated: {prd_desc}\n",
    "\n",
    "Firstly explain your response, followed by your final answer. You should follow the format\n",
    "Explanation: [Explanation], Answer: [Answer],\n",
    "where '[Answer]' can be one of the following:\n",
    "```\n",
    "unreadable\n",
    "poor readability\n",
    "fair readability\n",
    "good readability\n",
    "excellent readability\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "@observe(name=\"Readability Evaluation\")\n",
    "def generate_readability_score(trace_output):\n",
    "    prd_desc_readability = template_readability.format(prd_desc=trace_output)\n",
    "    # query = [f\"Rate the readability of product description: {traces_batch[1].output}\"]\n",
    "    readability_score = converse(\n",
    "            messages=[{\"role\": \"user\", \"content\": prd_desc_readability}], **MODEL_CONFIG[\"nova_pro\"]\n",
    "        )\n",
    "    explanation, score = readability_score.split(\"\\n\\n\")\n",
    "    return explanation, score\n",
    "\n",
    "\n",
    "print(f\"User query: {observations[0].input[1]['content']}\")\n",
    "print(f\"Model answer: {observations[0].output}\")\n",
    "explanation, score = generate_readability_score(observations[0].output)\n",
    "print(f\"Readability: {score}, Explanation: {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa4aea-f139-43db-8f81-ead648d05029",
   "metadata": {},
   "source": [
    "## 3. Add the evaluation to the trace\n",
    "\n",
    "Now that we have generated a readability score as well as a explanation, we can use the Langfuse client to add scores to existing traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f936b040-4fff-47ae-91ef-cd5c16db3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.score(\n",
    "    trace_id=traces_batch[0].id,\n",
    "    observation_id=observations[0].id,\n",
    "    name=\"readability\",\n",
    "    value=score,\n",
    "    comment=explanation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8fee5-a484-4a58-b0d0-72022bfc8f08",
   "metadata": {},
   "source": [
    "# Putting everything together\n",
    "\n",
    "We just saw how to do this for one trace, let's put it all together in a function to run it on all the traces collected in the last 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288a48c-8703-419c-99a7-a2fc1f3ac1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"Batch Readability Evaluation\")\n",
    "def batch_evaluate():\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=\"nova-user-1\",\n",
    "        session_id=\"nova-batch-evals-session\",\n",
    "        tags=[\"lab3.1\"],\n",
    "    )\n",
    "    traces_batch = langfuse.fetch_traces(\n",
    "        page=1,\n",
    "        limit=1,\n",
    "        tags=\"lab3.1\",\n",
    "        session_id=\"nova-batch-generation-session\",\n",
    "        from_timestamp=last_24_hours,\n",
    "        to_timestamp=datetime.now(),\n",
    "    ).data\n",
    "\n",
    "    observations = langfuse.fetch_observations(trace_id=traces_batch[0].id, type=\"GENERATION\").data\n",
    "\n",
    "    for observation in observations[-5:]: # Only evaluate the last 5 observations to save time\n",
    "        print(f\"Processing {observation.id}\")\n",
    "        if observation.output is None:\n",
    "            print(\n",
    "                f\"Warning: \\n Trace {observation.id} had no generated output, \\\n",
    "            it was skipped\"\n",
    "            )\n",
    "            continue\n",
    "        explanation, score = generate_readability_score(observation.output)\n",
    "        langfuse.score(\n",
    "            trace_id=traces_batch[0].id,\n",
    "            observation_id=observation.id,\n",
    "            name=\"readability\",\n",
    "            value=score,\n",
    "            comment=explanation,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e60d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_evaluate()\n",
    "\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8ed8e-bfb4-471d-a085-47863214b5b1",
   "metadata": {},
   "source": [
    "#### If your pipeline ran successfully, you should now see scores added to your traces\n",
    "\n",
    "![Langfuse Trace with score added for readability](./images/scored_trace.png \"Scored trace on langfuse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab301d0",
   "metadata": {},
   "source": [
    "### Congratuations\n",
    "You have successfully finished Lab 3.1.\n",
    "\n",
    "If you are at an AWS event, you can return to the workshop studio for additional instructions before moving into the next lab, where we will explore GenAI guardrails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
