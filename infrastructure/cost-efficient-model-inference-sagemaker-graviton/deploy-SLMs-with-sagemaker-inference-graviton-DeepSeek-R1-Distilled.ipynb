{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a6b096-a6eb-4698-80e6-4d62f386304b",
   "metadata": {},
   "source": [
    "# Deploy Small Language Models Cost-efficiently with Amazon SageMaker and AWS Graviton\n",
    "\n",
    "As organizations look to incorporate AI capabilities into their applications, Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks. [Amazon SageMaker AI](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html), AWS's fully managed machine learning service, provides a platform for deploying these ML models with multiple inference options, allowing organizations to optimize for cost, latency, and throughput. However, the computational requirements and costs associated with running these large, powerful LLMs can be prohibitive:\n",
    "\n",
    "- Traditional LLMs with billions of parameters require significant computational resources, often necessitating GPU instances with substantial memory.\n",
    "- This computational intensity and cost have led to growing interest in smaller, more efficient language models that can run on CPU infrastructure while still delivering good performance for specific use cases.\n",
    "- [AWS Graviton processors](https://aws.amazon.com/ec2/graviton/), specifically designed for cloud workloads, offer an optimal platform for running these quantized models, providing up to 50% better price performance compared to traditional x86-based instances for ML inference workloads.\n",
    "\n",
    "In this notebook, we'll demonstrate how to deploy a qauntized [DeepSeek R1 distilled 8B model](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF) on Amazon SageMaker AI using Graviton-based instances, highlighting the challenges of running large LLMs and the benefits of utilizing efficient language models on cost-optimized hardware.\n",
    "\n",
    "\n",
    "### Architecture and Components\n",
    "Our solution leverages Amazon SageMaker with AWS Graviton3 processors to run small language models cost-efficiently. The key components include:\n",
    "\n",
    "* Amazon SageMaker AI hosted endpoints for model serving\n",
    "* AWS  Graviton3-based instances (ml.c7g series) for computation\n",
    "* Llama.cpp  for CPU-optimized inference\n",
    "* Pre-quantized  GGUF format models\n",
    "\n",
    "[Llama.cpp](https://github.com/ggerganov/llama.cpp) uses GGUF, a special binary format for storing the model and metadata. Existing models need to be converted to GGUF format before they can be used for the inference. \n",
    "\n",
    "### Deployment Process\n",
    "To deploy your model on SageMaker with Graviton, you'll need to:\n",
    "\n",
    "1. Create  a Docker container compatible with ARM64 architecture\n",
    "2. Package  your model and inference code\n",
    "3. Create  a SageMaker model\n",
    "4. Configure  and launch an endpoint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7c868-2902-47c1-8a4b-189d803226a8",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "Install python packages and prepare environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844de6f1-de10-4660-a41a-7a88c771f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y zip\n",
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f7654-fc17-410e-a985-ecfc8d1945a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.session.Session(boto_session) # sagemaker session for interacting with different AWS APIs\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket()  # bucket to house model artifacts\n",
    "\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "print(f\"prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cfe99d-452a-461b-b7e5-17d315a3f12b",
   "metadata": {},
   "source": [
    "To run the model on Graviton processor, you need to use a docker container that supports the instance instance and has necessary packages installed. With Amazon SageMaker, you can package your own algorithms that can then be trained and deployed in the SageMaker environment. This notebook guides you through an example on how to extend one of our existing and predefined SageMaker deep learning framework containers. You can find a [list of available pre-built containers here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).\n",
    "\n",
    "By packaging an algorithm in a container, you can bring almost any code to the Amazon SageMaker environment, regardless of programming language, environment, framework, or dependencies. \n",
    "1. [Extending our PyTorch graviton containers](#Extending-our-PyTorch-containers)\n",
    "\n",
    "### Extending our PyTorch containers\n",
    "In this example we show how to package a prebuilt PyTorch container that supports Graviton instances, extending the SageMaker PyTorch container, with a Python example which works with the DeepSeek distilled model.\n",
    "\n",
    "#### How Amazon SageMaker runs your Docker container\n",
    "\n",
    "* Typically you specify a program (e.g. script) as an `ENTRYPOINT` in the Dockerfile, that program will be run at startup and decide what to do. The original `ENTRYPOINT` specified within the SageMaker PyTorch is [here](https://github.com/aws/deep-learning-containers/blob/1074667d84b69139eb91f1a2c5c6314269c1b792/pytorch/training/docker/2.5/py3/Dockerfile.cpu#L336).\n",
    "\n",
    "#### Running your container during training\n",
    "\n",
    "Currently, our SageMaker PyTorch container utilizes [console_scripts](http://python-packaging.readthedocs.io/en/latest/command-line-scripts.html#the-console-scripts-entry-point) to make use of the `train` command issued at training time. The line that gets invoked during `train` is defined within the setup.py file inside [SageMaker Training Toolkit](https://github.com/aws/sagemaker-training-toolkit/blob/e2d79421b1454f2e9b342c0b3366078a21b6eb18/setup.py#L94), our common SageMaker deep learning container framework. When this command is run, it will invoke the [trainer class](https://github.com/aws/sagemaker-training-toolkit/blob/master/src/sagemaker_training/cli/train.py) to run, which will finally invoke our [PyTorch container code](https://github.com/aws/sagemaker-pytorch-container/blob/master/src/sagemaker_pytorch_container/training.py) to run your Python file.\n",
    "\n",
    "A number of files are laid out for your use, under the `/opt/ml` directory:\n",
    "\n",
    "    /opt/ml\n",
    "    |-- input\n",
    "    |   |-- config\n",
    "    |   |   |-- hyperparameters.json\n",
    "    |   |   `-- resourceConfig.json\n",
    "    |   `-- data\n",
    "    |       `-- <channel_name>\n",
    "    |           `-- <input data>\n",
    "    |-- model\n",
    "    |   `-- <model files>\n",
    "    `-- output\n",
    "        `-- failure\n",
    "\n",
    "In this example, we will only using the inference contain as shown below.\n",
    "\n",
    "#### Running your container during hosting\n",
    "\n",
    "Hosting has a very different model than training because hosting is responding to inference requests that come in via HTTP. Currently, the SageMaker PyTorch containers [uses](https://github.com/aws/deep-learning-containers/blob/9fc00f0fa5a942304ac4fdb3812034c275dcfe72/pytorch/inference/docker/2.5/py3/Dockerfile.arm64.cpu#L151-L155) our [TorchServe](https://pytorch.org/serve/) to provide robust and scalable serving of inference requests:\n",
    "\n",
    "![Request serving stack](https://user-images.githubusercontent.com/880376/83180095-c44cc600-a0d7-11ea-97c1-23abb4cdbe4d.jpg)\n",
    "\n",
    "Amazon SageMaker uses two URLs in the container:\n",
    "\n",
    "* `/ping` receives `GET` requests from the infrastructure. Your program returns 200 if the container is up and accepting requests.\n",
    "* `/invocations` is the endpoint that receives client inference `POST` requests. The format of the request and the response is up to the algorithm. If the client supplied `ContentType` and `Accept` headers, these are passed in as well. \n",
    "\n",
    "The container has the model files in the same place that they were written to during training:\n",
    "\n",
    "    /opt/ml\n",
    "    `-- model\n",
    "        `-- <model files>\n",
    "\n",
    "#### Custom files available to build the container used in this example\n",
    "\n",
    "The `container` directory has all the components you need to extend the SageMaker PyTorch container to use as a sample algorithm:\n",
    "\n",
    "    .\n",
    "    |-- Dockerfile\n",
    "    `-- code\n",
    "        `-- inference.py\n",
    "        `-- requirements.txt\n",
    "\n",
    "Let's discuss each of these in turn:\n",
    "\n",
    "* __`Dockerfile`__ describes how to build your Docker container image for *inference*. More details are provided below.\n",
    "* __`build_and_push.sh`__ is a script that uses the Dockerfile to build your container images and then pushes it to ECR. We invoke the commands directly later in this notebook, but you can just copy and run the script for your own algorithms.\n",
    "* __`code`__ is the directory which contains our user code to be invoked.\n",
    "\n",
    "In this application, we install and/or update a few libraries for running Llama.cpp in Python\n",
    "\n",
    "The files that we put in the container are:\n",
    "\n",
    "* __`inference.py`__ is the program that implements our inference code (used only for inference container)\n",
    "* __`requirements.txt`__ is the text file that contains additional python packages which will be installed during deployment time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463af774-9e81-4fbd-b1c2-92552a3d497c",
   "metadata": {},
   "source": [
    "#### The inference Dockerfile\n",
    "\n",
    "The Dockerfile describes the image that we want to build. We start from the SageMaker PyTorch image as the base *inference* one. \n",
    "\n",
    "So the SageMaker PyTorch ECR image that supports Graviton in this case would be:\n",
    "* FROM 763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference-arm64:2.5.1-cpu-py311-ubuntu22.04-sagemaker\n",
    "\n",
    "Note: You can retrieve Dockerfile URIs with code such as:\n",
    "```\n",
    "from sagemaker import image_uris\n",
    "\n",
    "image_uris.retrieve('pytorch', 'us-east-1', '2.4', image_scope='inference_graviton')\n",
    "```\n",
    "\n",
    "Next, we install the required additional libraries and add the code that implements our specific algorithm to the container, and set up the right environment for it to run under.\n",
    "\n",
    "Let's look at the Dockerfile for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60ace1-bcef-4118-be98-d77ceac228d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM 763104351884.dkr.ecr.<region>.amazonaws.com/pytorch-inference-arm64:2.5.1-cpu-py311-ubuntu22.04-sagemaker\n",
    "\n",
    "RUN apt-get update && apt-get upgrade -y && apt-get install -y --no-install-recommends \\\n",
    "    ninja-build \\\n",
    "    cmake \\\n",
    "    libopenblas-dev \\\n",
    "    build-essential \\\n",
    "    && apt-get clean \\\n",
    "    && rm -rf /var/lib/apt/lists/* /tmp/*\n",
    "RUN python3 -m pip install --upgrade pip\n",
    "RUN pip uninstall ninja -y\n",
    "RUN python3 -m pip install --upgrade huggingface-hub pip pytest cmake scikit-build setuptools fastapi uvicorn sse-starlette pydantic-settings starlette-context\n",
    "ENV FORCE_CMAKE=1\n",
    "\n",
    "RUN CMAKE_ARGS=\"-DCMAKE_CXX_FLAGS='-mcpu=native -fopenmp' -DCMAKE_C_FLAGS='-mcpu=native -fopenmp'\" python3 -m pip install llama-cpp-python --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b294822-7251-4db9-a802-98743a9c8ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk -v region=\"$AWS_REGION\" '{gsub(/<region>/, region)}1' Dockerfile > Dockerfile.tmp && mv Dockerfile.tmp Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82e977-2345-4d3f-9c0b-990628920acb",
   "metadata": {},
   "source": [
    "### Permissions\n",
    "\n",
    "Running this notebook requires permissions in addition to the normal `SageMakerFullAccess` permissions. This is because it will use `codecommit` to create new repositories in Amazon ECR. You can add the below inline policy to the role that you used to start your notebook instance. There's no need to restart your notebook instance when you do this, the new permissions will be available immediately.\n",
    "```python\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor0\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"codebuild:BatchGetProjects\",\n",
    "                \"iam:PassRole\",\n",
    "                \"iam:DeleteRolePolicy\",\n",
    "                \"iam:ListAttachedRolePolicies\",\n",
    "                \"codebuild:ListBuilds\",\n",
    "                \"iam:CreateRole\",\n",
    "                \"iam:DeleteRole\",\n",
    "                \"codebuild:StartBuild\",\n",
    "                \"iam:PutRolePolicy\",\n",
    "                \"iam:ListRolePolicies\",\n",
    "                \"codebuild:CreateProject\",\n",
    "                \"codebuild:BatchGetBuilds\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639decc8-58dd-4c21-a99e-b8f176cbf7b5",
   "metadata": {},
   "source": [
    "### Building and registering the inference container\n",
    "\n",
    "The following shell code shows how to build the container image using `codebuild` and push the container image to ECR using `docker push`. The reason we need to use `codebuild` instead of the notebook locally is because the graviton supported docker containers need to be built using a graviton instance. Therefore, the `codebuild` provides the necessary compute environment for the docker build.\n",
    "\n",
    "This code looks for an ECR repository in the account you're using and the current default region. If the repository doesn't exist, the script will create it. In addition, since we are using the SageMaker PyTorch image as the base, we will need to retrieve ECR credentials to pull this public image.\n",
    "\n",
    "Note that role used by `codebuild` needs to have the permission to push images to the ECR registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6177f-5345-4264-81ea-5452d3ea0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2ff1e-3e16-4374-8e0c-86d101a7bcc5",
   "metadata": {},
   "source": [
    "### Writing your own inference script (inference.py)\n",
    "\n",
    "Given the use of a pre-packaged SageMaker PyTorch container, the only requirement to write an inference script is that it has to define the following template functions:\n",
    "- `model_fn()` reading the content of an existing model weights directory saved as a `tar.gz` in s3. We will use it to load the trained Model.\n",
    "- `input_fn()` used here simply to format the data receives from a request made to the endpoint.\n",
    "- `predict_fn()` calling the output of `model_fn()` to run inference on the output of `input_fn()`.\n",
    "\n",
    "Optionally a `output_fn()` can be created for inference formatting, using the output of `predict_fn()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f140b-67e7-4eaf-a00c-4d990abe20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "worker_count = os.environ.get('SAGEMAKER_MODEL_SERVER_WORKERS', cpu_count())\n",
    "model_file = os.environ.get('MODEL_FILE_GGUF', 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf')\n",
    "\n",
    "def input_fn(request_body, request_content_type, context):\n",
    "    return json.loads(request_body)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model=Llama(\n",
    "        model_path=f\"{model_dir}/{model_file}\",\n",
    "        verbose=False,\n",
    "        n_threads=cpu_count() // int(worker_count) # Graviton has 1 vCPU = 1 physical core\n",
    "    )\n",
    "    logging.info(\"Loaded model successfully\")\n",
    "    return model\n",
    "\n",
    "def predict_fn(input_data, model, context):\n",
    "    response = model.create_chat_completion(\n",
    "        **input_data\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def output_fn(prediction, response_content_type, context):\n",
    "    return json.dumps(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bfbfc-ae43-4a20-b6e5-8ba6b84b8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# hf_hub_download(repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\", filename=\"Llama-3.2-3B-Instruct-Q4_0.gguf\", local_dir='./code')\n",
    "from huggingface_hub import hf_hub_download\n",
    "file_name=\"DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf\"\n",
    "\n",
    "hf_hub_download(repo_id=\"bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF\", filename=file_name, local_dir='./code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad75b9d-c2b5-4422-ba91-185a3f6f5a73",
   "metadata": {},
   "source": [
    "Normally you would compress model files into a tar file however this can cause startup time to take longer due to having to download and untar large files. To improve startup times, SageMaker AI supports use of uncompressed files. This removes the need to untar large files.\n",
    "\n",
    "We upload all our files to an S3 prefix and then pass the location into the model with `\"CompressionType\": \"None\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c8ee5-7b8b-425c-8a95-ad79eb288b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = sagemaker_session.upload_data(f'./code/{file_name}', key_prefix=f'{prefix}-llama-cpp-model')\n",
    "script = model_data = sagemaker_session.upload_data('./code/inference.py', key_prefix=f'{prefix}-llama-cpp-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331f708-c725-4b3b-9a83-f30e90ab37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"s3://{sagemaker_session.default_bucket()}/{prefix}-llama-cpp-model/\"\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af9ece-922d-44fe-8be1-0a53b41ff292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data={\n",
    "                                \"S3DataSource\": {\n",
    "                                    \"S3Uri\": model_path,\n",
    "                                    \"S3DataType\": \"S3Prefix\",\n",
    "                                    \"CompressionType\": \"None\",\n",
    "                                }\n",
    "                            },\n",
    "                             role=role,\n",
    "                             env={\n",
    "                                 'MODEL_FILE_GGUF':file_name\n",
    "                             },\n",
    "                             image_uri=f\"{sagemaker_session.account_id()}.dkr.ecr.{region}.amazonaws.com/llama-cpp-python:latest\",\n",
    "                             model_server_workers=2\n",
    ")\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.c7g.12xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea258a-a83e-41f3-b46c-204fc9e50fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921cfc8-cd80-463b-a14d-f5db7ff92bde",
   "metadata": {},
   "source": [
    "We can use the SageMaker python SDK to invoke the endpoint as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b2fbd-980b-41ac-8962-d5b2d23c5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = {\n",
    "            'messages':[\n",
    "                {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"}\n",
    "            ],\n",
    "    'repeat_penalty': 1.1,\n",
    "    'temperature': 0.1\n",
    "}\n",
    "predictor.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f506fb-d8b0-4c48-b222-2bacb889e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8866b1fc-0b8e-45e0-899e-dcac8745adfe",
   "metadata": {},
   "source": [
    "You can also invoke the endpoint using the low level api which is the boto3 SageMaker client to invoke the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeaee5b-5350-49e7-9455-a9ead28234b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "print(response['Body'].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8685af-345a-4393-a9b7-39a23c79ce09",
   "metadata": {},
   "source": [
    "### Inference Recommender\n",
    "SageMaker Inference Recommender is the capability of SageMaker that reduces the time required to get machine learning (ML) models in production by automating load tests and optimizing model performance across instance types. You can use Inference Recommender to select a real-time inference endpoint that delivers the best performance at the lowest cost.\n",
    "\n",
    "Get started with Inference Recommender on SageMaker in minutes while selecting an instance and get an optimized endpoint configuration in hours, eliminating weeks of manual testing and tuning time.\n",
    "\n",
    "Inference Recommender uses metadata about your ML model to recommend the best instance types and endpoint configurations for deployment. You can provide as much or as little information as you'd like but the more information you provide, the better your recommendations will be.\n",
    "\n",
    "ML Frameworks: `TENSORFLOW, PYTORCH, XGBOOST, SAGEMAKER-SCIKIT-LEARN`\n",
    "\n",
    "ML Domains: `COMPUTER_VISION, NATURAL_LANGUAGE_PROCESSING, MACHINE_LEARNING`\n",
    "\n",
    "Example ML Tasks: `CLASSIFICATION, REGRESSION, IMAGE_CLASSIFICATION, OBJECT_DETECTION, SEGMENTATION, MASK_FILL, TEXT_CLASSIFICATION, TEXT_GENERATION, OTHER`\n",
    "\n",
    "Note: Select the task that is the closest match to your model. Chose `OTHER` if none apply.\n",
    "\n",
    "First, we need to create an archive that contains individual files that Inference Recommender can send to your SageMaker Endpoints. Inference Recommender will randomly sample files from this archive so make sure it contains a similar distribution of payloads you'd expect in production. Note that your inference code must be able to read in the file formats from the sample payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ebedc-9c69-4cd1-9e16-097699d2d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = predictor.serializer.serialize({'messages':[\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e33115-1bbd-4022-8808-a7d7716f7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_raw = json.dumps(raw)\n",
    "!echo {json_raw} > samplepayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24115542-d7e1-4e8e-a745-8bc64a97b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat samplepayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc8a86-a3cd-4548-9023-e460208d4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf payload.tar.gz samplepayload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a680031-5e88-42e0-843a-8eb2c57bf369",
   "metadata": {},
   "source": [
    "Next, we'll upload the packaged payload examples (payload.tar.gz) that was created above to S3. The S3 location will be used as input to our Inference Recommender job later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9790ab-6b06-4dce-b94e-eedfaa522de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = sagemaker_session.upload_data('./payload.tar.gz', key_prefix=f'{prefix}-llama-cpp-python-payload')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990e60f-10b8-45bc-be0f-93987d29b9e8",
   "metadata": {},
   "source": [
    "#### Run an Inference Recommendations Job\n",
    "\n",
    "The Python SDK method for Inference Recommender is `.right_size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b59a51-4ec4-4d8b-98b2-bc4f9685fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.parameter import CategoricalParameter\n",
    "from sagemaker.inference_recommender import Phase, ModelLatencyThreshold\n",
    "\n",
    "\n",
    "pytorch_model.right_size(payload, \n",
    "                         supported_content_types=['application/json'],\n",
    "                         supported_instance_types=['ml.c7g.8xlarge', 'ml.c7g.12xlarge'],\n",
    "                         framework='PYTORCH',\n",
    "                         job_duration_in_seconds=3600,\n",
    "                         hyperparameter_ranges=[{\n",
    "                             'instance_types': CategoricalParameter(['ml.c7g.8xlarge', 'ml.c7g.12xlarge']),\n",
    "                             'SAGEMAKER_MODEL_SERVER_WORKERS': CategoricalParameter([\"1\", \"2\", \"4\",])\n",
    "                         }],\n",
    "                         phases=[Phase(120, 1, 1), Phase(120, 2, 1), Phase(120, 7, 1)],\n",
    "                         traffic_type='PHASES',\n",
    "                         model_latency_thresholds=[ModelLatencyThreshold('P99', 50000)],\n",
    "                         max_invocations=120,\n",
    "                         log_level=\"Quiet\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6accbab9-1337-4068-a40a-67d12683ff5d",
   "metadata": {},
   "source": [
    "Once the inference recommender job has finished, you can navigate to the SageMaker AI console to check the job results.\n",
    "\n",
    "Each inference recommendation includes `InstanceType`, `InitialInstanceCount`, `EnvironmentParameters` which are tuned environment variable parameters for better performance. We also include performance and cost metrics such as `MaxInvocations`, `ModelLatency`, `CostPerHour` and `CostPerInference`. We believe these metrics will help you narrow down to a specific endpoint configuration that suits your use case. \n",
    "\n",
    "Example:   \n",
    "\n",
    "If your motivation is overall price-performance with an emphasis on throughput, then you should focus on `CostPerInference` metrics  \n",
    "If your motivation is a balance between latency and throughput, then you should focus on `ModelLatency` / `MaxInvocations` metrics\n",
    "\n",
    "| Metric | Description |\n",
    "| --- | --- |\n",
    "| ModelLatency | The interval of time taken by a model to respond as viewed from SageMaker. This interval includes the local communication times taken to send the request and to fetch the response from the container of a model and the time taken to complete the inference in the container. <br /> Units: Microseconds |\n",
    "| MaximumInvocations | The maximum number of InvokeEndpoint requests sent to a model endpoint. <br /> Units: None |\n",
    "| CostPerHour | The estimated cost per hour for your real-time endpoint. <br /> Units: US Dollars |\n",
    "| CostPerInference | The estimated cost per inference for your real-time endpoint. <br /> Units: US Dollars |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab4070c-7f7b-4386-8627-e1c66eee4722",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e31e2-987b-4c5c-83b1-eb88d9954563",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2da3c-390f-49ad-accf-41123316ea68",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [How Amazon SageMaker interacts with your Docker container for training](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)\n",
    "- [How Amazon SageMaker interacts with your Docker container for inference](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)\n",
    "- [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk)\n",
    "- [Dockerfile](https://docs.docker.com/engine/reference/builder/)\n",
    "- [SageMaker multi-model endpoint bring your own container](https://github.com/aws/amazon-sagemaker-examples/tree/f671af53c3f7c77172e5803a4ff5a3ea8672ecb6/%20%20%20%20%20deploy_and_monitor/sm-multi_model_endpoint_bring_your_own_container)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
